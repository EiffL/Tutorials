{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EiffL/Tutorials/blob/master/InverseProblems/DeepInverseProblemsTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2021-2023 Francois Lanusse.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n"
      ],
      "metadata": {
        "id": "bUh0UQMKj9E4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDcRQNGAhv4r"
      },
      "source": [
        "# Introduction to Differentiable Programming and Deep Learning Priors in Jax\n",
        "\n",
        "Author:\n",
        " - [@EiffL](https://github.com/EiffL) (Fancois Lanusse)\n",
        "\n",
        "## Overview\n",
        "\n",
        "The overall **aim of this tutorial is to solve image inverse problem (denoising, deconvolution, inpainting) in a fully Bayesian way** i.e. drawing from a posterior distribution, and using a deep generative model as a prior.\n",
        "\n",
        "More concretely, **we will consider a deconvolution problem** where one aims to recover a high quality image from a low resolution image degraded by an instrumental response. The following figure provides an illustration of the problem:\n",
        "\n",
        "![](https://www.researchgate.net/profile/Alan-Heavens/publication/1910339/figure/fig1/AS:394636699947010@1471100129570/Illustration-of-the-forward-problem-The-upper-panels-show-how-the-original-galaxy-image.png)\n",
        "\n",
        "What we will have access to is an observed galaxy on the right of the chain, and we will try to recover the high resolution image before PSF convolution.\n",
        "\n",
        "\n",
        "We will use two datasets as the basis for this challenge:\n",
        "- Galaxies from the Hubble Space Telescope COSMOS survey, described for instance [here](https://arxiv.org/pdf/1308.4982.pdf#page=24) under `REAL GALAXY DATASET`.\n",
        "- Galaxies of the [HSC Survey](https://hsc-release.mtk.nao.ac.jp/doc/), taken with the ground-based Subaru telescope\n",
        "\n",
        "This tutorial will be split into 2 parts:\n",
        "- In this first part, we will be discovering the datasets and tools needed for forward modeling.\n",
        "- In the second part, we will be use generative modeling to solve this inverse problem.\n",
        "\n",
        "### Learning objectives\n",
        "\n",
        "Through this tutorial, you will put into practice the following concepts:\n",
        "\n",
        "  - Build a forward model using Jax\n",
        "  - Build a likelihood using TensorFlow Probability\n",
        "  - Use a generative model to impose a data-driven prior\n",
        "  - Retrieve maximum likelihood and a posterior solutions by optimization\n",
        "  - Retrieve full posterior distributions by variational inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies and accessing data"
      ],
      "metadata": {
        "id": "0ktLeF7bjnbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/EiffL/Quarks2CosmosDataChallenge.git ChainConsumer\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "!mkdir galsim\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())"
      ],
      "metadata": {
        "id": "p_56Uqv6h0QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticating and mounting cloud data storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcsfuse --implicit-dirs galsim galsim"
      ],
      "metadata": {
        "id": "rypc1fA8iK9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrcxfNpEhv4s"
      },
      "source": [
        "## Before we begin... A few words about JAX\n",
        "\n",
        "What's the best Deep Learning framework? TensorFlow? PyTorch? Wrong: all you ever want is JAX :-)\n",
        "\n",
        "![](https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png)\n",
        "\n",
        "JAX = NumPy + autograd + XLA\n",
        "\n",
        "In other words, it looks like pure NumPy, but is automatically differentiable, and runs on XLA (i.e. GPU accelerated). Checkout the [full documentation](https://jax.readthedocs.io/en/latest/index.html) to discover the many awesome features JAX has to offer.\n",
        "\n",
        "#### Autodiff\n",
        "\n",
        "For our purposes, the most interesting feature of JAX will be automatic differentiation, which is performed using `jax.grad`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ALu_s-Jhv4s"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb5_06BRhv4t"
      },
      "outputs": [],
      "source": [
        "# Let's build a function of x that returns a scalar\n",
        "def f(x):\n",
        "    y = 2*x+1\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvRuEZu3hv4u"
      },
      "outputs": [],
      "source": [
        "# To get the derivative of this function, I simply use jax.grad\n",
        "df_dx = jax.grad(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p63iXDahv4u"
      },
      "outputs": [],
      "source": [
        "df_dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oN9MfsVhv4u"
      },
      "source": [
        "`jax.grad` has *transformed* my function `f` into a new function that comptutes its gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntSK2Oefhv4u"
      },
      "outputs": [],
      "source": [
        "u = jnp.linspace(0,1)\n",
        "plot(u, f(u), label='f')\n",
        "plot(u,jax.vmap(df_dx)(u), label='df/dx')\n",
        "legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kqbIyaPhv4u"
      },
      "source": [
        "#### Vectorization with `vmap`\n",
        "\n",
        "Another awesome feature of JAX is the ability to batch any function using the `jax.vmap` transformation. Say you have a function that works on a single example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVJ9OkAxhv4v"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    # Expect x to be of shape [16, 16]\n",
        "    nx, ny = x.shape\n",
        "    x = x + jnp.ones([nx,ny])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIP0aKdbhv4v"
      },
      "outputs": [],
      "source": [
        "x = jnp.zeros([16,16])\n",
        "f(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjNNW1wHhv4v"
      },
      "outputs": [],
      "source": [
        "# But this function wouldn't work if I had a batch of images\n",
        "x = jnp.zeros([1,16,16])\n",
        "f(x).shape # THIS SHOULD FAIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjWxImKyhv4v"
      },
      "outputs": [],
      "source": [
        "# But I can use jax magic\n",
        "f_batched = jax.vmap(f)\n",
        "\n",
        "x = jnp.zeros([1,16,16])\n",
        "f_batched(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBuPi28Phv4v"
      },
      "source": [
        "Tada!\n",
        "\n",
        "#### JIT compilation\n",
        "\n",
        "The final super useful thing to know about JAX is that you can JIT (Just In Time) compile any function, they will be compiled as an XLA graph and completely bypass Python to run directly under XLA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOKnoiKGhv4v"
      },
      "outputs": [],
      "source": [
        "def f(x, A):\n",
        "    return A.dot(x)\n",
        "\n",
        "f_jitted = jax.jit(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCM3rZqAhv4v"
      },
      "outputs": [],
      "source": [
        "x = randn(128)\n",
        "A = randn(128,128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxFjYbXYhv4v"
      },
      "outputs": [],
      "source": [
        "f_jitted(x,A);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je65LTIwhv4w"
      },
      "source": [
        "On this example, jitting won't make much of a difference because it is  a tiny function, but we will see later that we can JIT the computation of the loss and update of a full neural network, which then becomes lightning fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmK-Gl-thv4w"
      },
      "source": [
        "## Step I: Loading the data\n",
        "\n",
        "In this section, we load the datasets for our space-based and ground-based images. Both datasets are conviently accessible as [TensorFlow Datasets](https://www.tensorflow.org/datasets) datasets, you simply need the following imports to access them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idebrU00hv4w"
      },
      "outputs": [],
      "source": [
        "import quarks2cosmos.datasets\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XvtB4FChv4w"
      },
      "source": [
        "#### COSMOS Dataset\n",
        "\n",
        "The COSMOS dataset is built from the GalSim COSMOS sample (see [here](https://galsim-developers.github.io/GalSim/_build/html/real_gal.html) for details), it contains postage stamps of real galaxies,  **convolved with an isotropized HST PSF** which is the same for all galaxies. These images are thus completely standardized, and the PSF doesn't vary from one object to the other.\n",
        "\n",
        "Properties of COSMOS stamps:\n",
        "- TRAIN and TEST splits with 40000 and 10000 images respectively\n",
        "- Constant isotropic PSF\n",
        "- Pixel scale 0.03\n",
        "- Postage stamp size: 101x101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFAVe3HMhv4w"
      },
      "outputs": [],
      "source": [
        "dset_cosmos = tfds.load(\"Cosmos/23.5\", split=tfds.Split.TRAIN,\n",
        "                        data_dir='galsim/tensorflow_datasets') # Load the TRAIN split\n",
        "dset_cosmos = dset_cosmos.as_numpy_iterator()                  # Convert the dataset to numpy iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIUdGZb6hv4w"
      },
      "outputs": [],
      "source": [
        "# Extract a new example from the dataset\n",
        "cosmos = next(dset_cosmos)\n",
        "cosmos = next(dset_cosmos)\n",
        "\n",
        "figure(figsize=[10,5])\n",
        "subplot(121)\n",
        "imshow(cosmos['image'],cmap='gray')\n",
        "title('Galaxy')\n",
        "subplot(122)\n",
        "imshow(cosmos['psf'],cmap='gray')\n",
        "title('PSF');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lgrSw75hv4w"
      },
      "source": [
        "#### HSC Dataset\n",
        "\n",
        "The HSC galaxies are drawn from the PDR2 data release and extracted using the excellent [unagi](https://github.com/dr-guangtou/unagi) tool. Details of how these objects are selected can be found in this accompanying [notebook](HSCDataPreparation.ipynb), but essentially these are i-band stamps of galaxies with imag $\\in [21, 23.5]$, also verifying the `forced.i_pixelflags_interpolatedcenter` condition, meaning they contain a number of interpolated pixels which intersect the center of the object. The stamps are at resolution 0.168 arcsec, and the PSF is provided for each galaxy.\n",
        "\n",
        "Properties of HSC stamps:\n",
        "- Single TRAIN split of ~10000 examples\n",
        "- PSF, pixel masks, and variance map provided for every image\n",
        "- Pixel scale: 0.168\n",
        "- Postage stamp size: 41x41"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCxLlFhqhv4w"
      },
      "outputs": [],
      "source": [
        "dset_hsc = tfds.load(\"HSC\", split=tfds.Split.TRAIN,\n",
        "                     data_dir='galsim/tensorflow_datasets')\n",
        "dset_hsc = dset_hsc.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXOwun5Dhv4w"
      },
      "outputs": [],
      "source": [
        "# Extract a new example from the dataset\n",
        "hsc = next(dset_hsc)\n",
        "\n",
        "figure(figsize=[20,5])\n",
        "subplot(141)\n",
        "imshow(hsc['image'],cmap='gray')\n",
        "title('Galaxy')\n",
        "subplot(142)\n",
        "imshow(hsc['psf'],cmap='gray')\n",
        "title('PSF')\n",
        "subplot(143)\n",
        "imshow(hsc['mask'] == 44,cmap='gray')\n",
        "title('Interpolated pixels')\n",
        "subplot(144)\n",
        "imshow(hsc['variance'],cmap='gray')\n",
        "title('Variance plane');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAe8CxGOhv4w"
      },
      "source": [
        "## Step II: Building a forward model\n",
        "\n",
        "\n",
        "\n",
        "We only provides the fundamentals here, and we encourage the interested reader to directly reach out to the tutorial organizers to learn more :-)\n",
        "\n",
        "The problems we will be addressing in this tutorial can be written as the following:\n",
        "\n",
        "$$y = \\Pi \\ast \\left(  \\mathbf{P} \\ (\\Pi_{HST}^{-1} \\ast x ) \\right) + n \\qquad \\mbox{with} \\qquad n \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
        "where $y$ are the measurements and where:\n",
        "- $x$ would be a space-based HST galaxy image we would like to recover\n",
        "- $\\Pi_{HST}$ is the HST PSF\n",
        "- $\\mathbf{P}$ is a resampling operator to go from the pixel scale of HST to that of HSC images\n",
        "- $\\Pi$ is a given HSC PSF\n",
        "- $\\sigma$ is the noise variance of HSC observations.  \n",
        "\n",
        "Typically, one would use the [GalSim software](https://github.com/GalSim-developers/GalSim) to perform these manipulations (convolution/deconvolution/resampling) but GalSim is not differentiable :-(\n",
        "\n",
        "For the purpose of this data challenge we provide JAX equivalent functions as part of the `quarks2cosmos.galjax` and this will allow us to write a differentiable forward model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7V6ur1thv4w"
      },
      "outputs": [],
      "source": [
        "from quarks2cosmos import galjax as gj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QKRubUPhv4w"
      },
      "outputs": [],
      "source": [
        "def simulate_hsc(x, in_psf, out_psf):\n",
        "    \"\"\" This function will simulate an image at HSC resolution given an image at HST resolution,\n",
        "    accounting for input PSF and convolving by output PSF\n",
        "    Args:\n",
        "        x: HST resolution image (MUST BE ODD SIZE!!!!)\n",
        "        in_psf: HST PSF\n",
        "        out_psf: HSC PSF\n",
        "    Returns:\n",
        "        y: HSC simulated image of size [41,41]\n",
        "    \"\"\"\n",
        "    y = gj.deconvolve(x, in_psf)         # Deconvolve by input PSF\n",
        "    y = gj.kresample(y, 0.03, 0.168, 41) # Resample image to HSC grid\n",
        "    y = gj.convolve(y,  out_psf)         # Reconvolve by HSC PSF\n",
        "    return 2.587*y                       # Conversion factor for the flux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6s0itw8hv4w"
      },
      "outputs": [],
      "source": [
        "# Let's apply it to our input images\n",
        "im = simulate_hsc(cosmos['image'], cosmos['psf'], hsc['psf'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyonMmTShv4w"
      },
      "outputs": [],
      "source": [
        "subplot(121)\n",
        "imshow(cosmos['image'], cmap='gray')\n",
        "title('As seen from Hubble')\n",
        "subplot(122)\n",
        "imshow(im, cmap='gray')\n",
        "title('As seen from Subaru');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1zMK6bohv4x"
      },
      "source": [
        "So far our model only generates an HSC image without noise, but we are provided with a noise variance map for each observation, so we can actually write down a likelihood function.\n",
        "\n",
        "#### Data likelihood using TensorFlow Probability\n",
        "\n",
        "This is an opportunity to introduce the awesome [TensorFlow Probabilty](https://www.tensorflow.org/probability) package, which also works with JAX ;-) with the following imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iftqI6Vehv4x"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXXp4l00hv4x"
      },
      "source": [
        "As mentioned in the model above, we assume a Gaussian likelihood for the observations:\n",
        "\n",
        "$$ p(y | x) = \\mathcal{N}(y ; f(x), \\Sigma) $$\n",
        "\n",
        "where $f$ is our forward model defined above, and $\\Sigma$ is our noise covariance, which we assume diagonal here. In TFP, we can create this likelihood very easily as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kVtKfcdhv4x"
      },
      "outputs": [],
      "source": [
        "likelihood = tfd.Independent(tfd.Normal(loc=simulate_hsc(cosmos['image'], cosmos['psf'], hsc['psf']),\n",
        "                                        scale=jnp.sqrt(hsc['variance'])),\n",
        "                             reinterpreted_batch_ndims=2) # This is to make sure TFP understand we have a 2d image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF7v0EdMhv4x"
      },
      "outputs": [],
      "source": [
        "likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF6TRugChv4x"
      },
      "source": [
        "Once you have this likelihood, you can for instance sample from it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Jf6IELhv4x"
      },
      "outputs": [],
      "source": [
        "im_noise = likelihood.sample(seed=jax.random.PRNGKey(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_l05caNhv4x"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[15,5])\n",
        "subplot(131)\n",
        "imshow(cosmos['image'], cmap='gray')\n",
        "title('As seen from Hubble')\n",
        "subplot(132)\n",
        "imshow(im, cmap='gray')\n",
        "title('As seen from Subaru')\n",
        "subplot(133)\n",
        "imshow(im_noise, cmap='gray')\n",
        "title('As seen from Subaru + Noise')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyKTOFeShv4y"
      },
      "source": [
        "Or evaluate the likelihood at any given point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn9Pi6ehhv4y"
      },
      "outputs": [],
      "source": [
        "likelihood.log_prob(im_noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dggm02Y3hv4y"
      },
      "source": [
        "## Step III: Solving an inverse problem by optimization\n",
        "\n",
        "Now that we have the ability to write a forward model and evaluate a likelihood, we can get started on trying to solve our inverse problem.\n",
        "\n",
        "We will start by trying to recover a maximum likelihood solution\n",
        "$$ \\hat{x} = \\arg \\max_{x} \\log p(y | x) $$\n",
        "\n",
        "Let's simulate some observations that will act as our observables $y$ and for which we will know the truth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpDjCJpwhv4y"
      },
      "outputs": [],
      "source": [
        "# Let's make sure we use the correct image for the rest of this tutorial\n",
        "dset_cosmos = tfds.load(\"Cosmos/23.5\", split=tfds.Split.TRAIN,\n",
        "                        data_dir='galsim/tensorflow_datasets').as_numpy_iterator()\n",
        "cosmos = [next(dset_cosmos) for i in range(2)][-1]\n",
        "\n",
        "# We will use this new galaxy as are reference here\n",
        "x_true = cosmos['image']\n",
        "# And we apply the same logic as above to create a sample from the corresponding HSC likelihood\n",
        "y_obs = tfd.Independent(tfd.Normal(loc=simulate_hsc(cosmos['image'], cosmos['psf'], hsc['psf']),\n",
        "                                    scale=jnp.sqrt(hsc['variance'])),\n",
        "                             reinterpreted_batch_ndims=2).sample(seed=jax.random.PRNGKey(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60c8fAjshv4y"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[10,5])\n",
        "subplot(121)\n",
        "imshow(x_true, cmap='gray')\n",
        "title('Hubble image to recover')\n",
        "subplot(122)\n",
        "imshow(y_obs, cmap='gray')\n",
        "title('Observed image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk6K7DRLhv4y"
      },
      "source": [
        "#### Writing a log likelihood function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDk0-vzDhv4y"
      },
      "outputs": [],
      "source": [
        "def log_prob(x):\n",
        "    \"\"\" Returns the value of the log likelihood of the observed data for a givem x\n",
        "    \"\"\"\n",
        "    # EXERCISE: write down the likelihood of the observations using TFP\n",
        "    # Hint: See in the previous section how we built a TFP distribution object to\n",
        "    # represent the likelihood\n",
        "    likelihood = tfd.Independent(tfd.Normal(loc=  #..... Complete here\n",
        "                                           ,scale= # .... Complete here\n",
        "                                            )),\n",
        "                             reinterpreted_batch_ndims=2)\n",
        "    return likelihood.log_prob(y_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_dKx4wOhv4y"
      },
      "source": [
        "#### Creating an optimizer\n",
        "Once we have a likelihood, we now want to optimize it, for this we will use the optax library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3ZVG3vGhv4y"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "optimizer = optax.adam(0.01) # Instantiate an ADAM optimizer\n",
        "\n",
        "# Create a variable to store the solution\n",
        "x = jnp.zeros([101, 101])\n",
        "\n",
        "# Initialize the optimizer\n",
        "opt_state = optimizer.init(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFdN46oKhv4y"
      },
      "source": [
        "#### Writing an update function\n",
        "\n",
        "The way optax and most JAX neural network libraries work is that the optimizer provides a function that computes how to update the parameters given the gradients of the loss function. One iteration of the optimization will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uIMq3Gchv4y"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def update(x, opt_state):\n",
        "    \"\"\" Computes update to parameters x\n",
        "    \"\"\"\n",
        "    logp, grads = jax.value_and_grad(log_prob)(x)              # Takes the gradients of the likelihood\n",
        "    updates, opt_state = optimizer.update(-grads, opt_state)   # Computes ADAM update to maximize likelihood\n",
        "    x = optax.apply_updates(x, updates)                        # Apply update to parameters\n",
        "    return logp, x, opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y1qZdQihv4y"
      },
      "source": [
        "#### Running the optimization loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JV8fQgchv4z"
      },
      "outputs": [],
      "source": [
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgRGDRc4hv4z"
      },
      "outputs": [],
      "source": [
        "# This runs the optimization loop\n",
        "for i in range(200):\n",
        "    logp, x, opt_state = update(x, opt_state)\n",
        "    losses.append(logp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CosdkZ_Shv4z"
      },
      "outputs": [],
      "source": [
        "loglog(abs(jnp.array(losses)))\n",
        "xlabel('step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGp7nDsZhv4z"
      },
      "outputs": [],
      "source": [
        "# Let's look at the solution!\n",
        "figure(figsize=[15,5])\n",
        "subplot(141)\n",
        "imshow(x, cmap='gray')\n",
        "title('Current Solution')\n",
        "subplot(142)\n",
        "imshow(simulate_hsc(x, cosmos['psf'], hsc['psf']), cmap='gray')\n",
        "title('As seen from Subaru')\n",
        "subplot(143)\n",
        "imshow(y_obs, cmap='gray')\n",
        "title('Observations')\n",
        "subplot(144)\n",
        "imshow(y_obs - simulate_hsc(x, cosmos['psf'], hsc['psf']), cmap='gray')\n",
        "title('Residuals');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpXmqaYshv4z"
      },
      "source": [
        "And there you have it, we have found **a** solution, which fits the observations very well! Now, is it a good solution? Given that the inverse problem is very ill-posed and that we have no prior, we get a solution that doesn't appear very physical."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step IV: Getting our hands on a Deep Generative Model\n",
        "\n",
        "Now that we know how to build the forward model, we can work on building a good prior model for our problem.\n",
        "\n",
        "The idea is to use a Deep Generative Model that would have been trained on images of space-based galaxies to act as our prior. For the purpose of this tutorial, we will use a **Variational Auto-Encoder** model that has been fitted to the COSMOS sample.\n",
        "\n",
        "**Note**: The details of the architecture of this model are beyond the scope of this tutorial. We direct the interested reader to this notebook:\n",
        "  - [Guided Data Challenge Part II: Generative Modeling](https://github.com/EiffL/Quarks2CosmosDataChallenge/blob/colab/notebooks/PartII-GenerativeModels.ipynb): Describes how to build and train the VAE on COSMOS data.\n"
      ],
      "metadata": {
        "id": "BGJCNsjCyq4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports, useful functions, and parameter loading [Run me]\n",
        "\n",
        "# Specific models built by EiffL\n",
        "from quarks2cosmos.models.vae import Decoder\n",
        "from quarks2cosmos.models.flow import AffineFlow\n",
        "# Restore model parameters\n",
        "import pickle\n",
        "import haiku as hk\n",
        "\n",
        "with open('/content/galsim/model-50000.pckl', 'rb') as file:\n",
        "    params_vae, state_vae, _ = pickle.load(file)\n",
        "with open('/content/galsim/model-20000.pckl', 'rb') as file:\n",
        "    params_flow, _ = pickle.load(file)\n",
        "params_generative_model = hk.data_structures.merge(params_vae, params_flow)\n",
        "\n",
        "def generative_model_fn(z):\n",
        "  \"\"\" Transforms a latent variable z into a morphology image.\n",
        "  Inputs:\n",
        "    z: a Normal variable of size [32]\n",
        "  Outputs:\n",
        "    x: an image of size [101,101] with pixels of 0.03 arcsec\n",
        "  \"\"\"\n",
        "  z = jnp.atleast_2d(z).astype('float32')\n",
        "  # Transform from Gaussian space to VAE latent space\n",
        "  z1 = AffineFlow()().bijector.forward(z)\n",
        "  # Decode sample with decoder\n",
        "  likelihood = Decoder()(z1, is_training=False)\n",
        "  # Return likelihood mean\n",
        "  return likelihood.mean().squeeze()\n",
        "\n",
        "gen_model_fn = hk.without_apply_rng(hk.transform_with_state(generative_model_fn))\n",
        "\n",
        "generative_model = jax.jit(lambda z: gen_model_fn.apply(params_generative_model, state_vae, z)[0])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Yu32oAp1ylrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generative model we will be using takes as an input a latent variable $z$ of size 32, and returns an image $x$ of size 101x101 with pixels of size 0.03 arcsec.\n",
        "\n",
        "To sample from the model:"
      ],
      "metadata": {
        "id": "cRHNnYyO0QsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw a random Normal\n",
        "z = tfd.MultivariateNormalDiag(jnp.zeros(32), jnp.ones(32)).sample(9, seed=jax.random.PRNGKey(10))\n",
        "\n",
        "# Run it through the forward model\n",
        "x = generative_model(z)"
      ],
      "metadata": {
        "id": "XA3t6qmx0Qd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And here is what each sample looks like:\n",
        "figure(figsize=(10,10))\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        subplot(3,3,i+3*j+1)\n",
        "        imshow(x[i+3*j],cmap='gray')\n",
        "        axis('off')"
      ],
      "metadata": {
        "id": "ndPzdo71yloK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty cool :-) These look like galaxies"
      ],
      "metadata": {
        "id": "OWLljmvS0xk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step V: Maximum A Posterior Solution under a data-driven prior\n",
        "\n",
        "With this generative model in hands, we can redefine our forward model to include the morphology model in the following way:\n",
        "\n",
        "$$y = \\Pi \\ast \\left(  \\mathbf{P} \\ (\\Pi_{HST}^{-1} \\ast f_\\theta(z) ) \\right) + n \\qquad \\mbox{with} \\qquad n \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
        "where $y$ are the measurements and where:\n",
        "- **$z$ is the latent variable of our forward model**, this is now the variable we are interested in inferring.\n",
        "- **$f_\\theta$ is the generator network of the VAE**\n",
        "- $\\Pi_{HST}$ is the HST PSF\n",
        "- $\\mathbf{P}$ is a resampling operator to go from the pixel scale of HST to that of HSC images\n",
        "- $\\Pi$ is a given HSC PSF\n",
        "- $\\sigma$ is the noise variance of HSC observations.  \n",
        "\n",
        "Contrary to the previous section where we did not have a good prior for $x$, by construction of the generative model, we now can impose a standard Gaussian prior on $z$:\n",
        "\n",
        "$$p(z) = \\mathcal{N}(0, I)$$\n",
        "\n",
        "\n",
        "**We now have all the tools for trying to perform Maximum A Posterior (MAP) inference** for our inverse problem, i.e.:\n",
        "\n",
        "$$\\hat{z} = \\arg \\max_{z} \\log p(y | z) + \\log p(z) $$\n",
        "\n",
        "In order to achieve this, you will need to put together the following elements:\n",
        "\n",
        "- Combine the physical forward model with generative model for an end-to-end forward model going from latent variable $z$ to HSC image.\n",
        "- Write a function that computes the log posterior for a given $z$\n",
        "- Use the tools from day I to do the optmization and recover a solution\n"
      ],
      "metadata": {
        "id": "P8GwFY3R1FMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To keep things contained, let's redefine the likelihood function of our data here\n",
        "# but this time redefined in terms of our latent variable z\n",
        "def log_likelihood(z):\n",
        "\n",
        "  # EXERCISE: Here we want to use the generative model to turn z into x\n",
        "  # Hint: Look at how we generated images from the generative model in the previous section\n",
        "  x = # ..... Compltete this line\n",
        "\n",
        "  # And we reuse our previous likelihood\n",
        "  likelihood = tfd.Independent(tfd.Normal(simulate_hsc(x, cosmos['psf'], hsc['psf']),\n",
        "                                            jnp.sqrt(hsc['variance'])),\n",
        "                             reinterpreted_batch_ndims=2)\n",
        "  return likelihood.log_prob(y_obs)"
      ],
      "metadata": {
        "id": "VknFIRYT039N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need to define the log prior on the latent variable z of our problem\n",
        "def log_prior(z):\n",
        "  # EXERCISE: Build a distribution object to model the prior on z\n",
        "  # and use it to return the log probability of z under the prior\n",
        "  # Hint: You can use tfd.MultivariateNormalDiag to represent a Standard Normal\n",
        "  # multidimensional distribution\n",
        "  prior = # to complete ...\n",
        "  return  # to complete ... should be similar to the output of the function above"
      ],
      "metadata": {
        "id": "OTwwLH0b036Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining these two, we can write down the log posterior\n",
        "def log_posterior(z):\n",
        "  # EXERCISE: return the log posterior of the problem\n",
        "  return # ....."
      ],
      "metadata": {
        "id": "-cusEvO0033X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optax.adam(0.002)\n",
        "\n",
        "# Create a parameter\n",
        "z = jnp.zeros([32])\n",
        "\n",
        "opt_state = optimizer.init(z)"
      ],
      "metadata": {
        "id": "kpN2kke6030K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def update(z, opt_state):\n",
        "    logp, grads = jax.value_and_grad(log_posterior)(z)\n",
        "    updates, opt_state = optimizer.update(-grads, opt_state)\n",
        "\n",
        "    # Apply gradient descent\n",
        "    z = optax.apply_updates(z, updates)\n",
        "\n",
        "    return logp, z, opt_state"
      ],
      "metadata": {
        "id": "KYe1U_8g03xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []"
      ],
      "metadata": {
        "id": "gHQ4bRLK5-Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5000):\n",
        "    logp, z, opt_state = update(z, opt_state)\n",
        "    losses.append(logp)"
      ],
      "metadata": {
        "id": "0cGiyon45-Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(np.array(losses)[500:])"
      ],
      "metadata": {
        "id": "2fB7SxDC59_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's reconstruct x from z and visualize the solution\n",
        "x = generative_model(z)\n",
        "\n",
        "figure(figsize=[15,5])\n",
        "subplot(141)\n",
        "imshow(x, cmap='gray')\n",
        "title('Solution')\n",
        "subplot(142)\n",
        "imshow(simulate_hsc(x, cosmos['psf'], hsc['psf']), cmap='gray')\n",
        "title('As seen from Subaru')\n",
        "subplot(143)\n",
        "imshow(y_obs, cmap='gray')\n",
        "title('Observations')\n",
        "subplot(144)\n",
        "imshow((y_obs - simulate_hsc(x, cosmos['psf'], hsc['psf'])), cmap='gray')\n",
        "title('Residuals');"
      ],
      "metadata": {
        "id": "_FuUShDU598x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How nice is that :-) Our reconstructed galaxy actually looks like a galaxy \\o/\n",
        "\n",
        "\n",
        "But now, let's compare it the orginal galaxy we used as an input:"
      ],
      "metadata": {
        "id": "VbuNKLrj-VK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=[15,5])\n",
        "subplot(131)\n",
        "imshow(x_true, cmap='gray')\n",
        "title('True Hubble Image')\n",
        "\n",
        "subplot(132)\n",
        "imshow(x, vmax=x_true.max(), cmap='gray')\n",
        "title('Reconstructed MAP solution')\n",
        "\n",
        "subplot(133)\n",
        "imshow(x_true - x, cmap='gray')\n",
        "title('Residuals of the MAP reconstruction');"
      ],
      "metadata": {
        "id": "5CwHhkvh595z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the MAP solution is not the true answer, despite providing a satisfying fit to the data and being likely under the prior.\n",
        "\n",
        "One of the implications here, is that this reconstructed image is not the full story, **it does not tell us about the uncertainty on the solution.**\n",
        "\n",
        "To go one step further, instead of looking for the MAP solution, we can try to sample from the full posterior distribution $p(z|y)$. Let's see how to do that in the next section."
      ],
      "metadata": {
        "id": "Y9KwETNe_CLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step VI: Full Posterior Estimation by Variational Inference\n",
        "\n",
        "\n",
        "In this final section, we will be combining everything we learned today into estimating the full posterior solution of our deconvolution problem. In other words, we now want to estimate:\n",
        "$$ p (z|y) \\propto p(y | z) p(z)$$\n",
        "\n",
        "If we can estimate this posterior, it captures the full uncertainty on the solution under the specific prior assumption that we have made. However, in general, it is slow and complicated to estimate posteriors in high dimensions by traditional MCMC methods. In this tutorial, we will see how to solve the problem with much fast **Variational Inference** (VI).\n",
        "\n",
        "The idea of VI, is to use a parametric model $q_\\phi$ to approximate the posterior distribution $p(\\theta | x)$. You need two things:\n",
        "- a tractable and flexible parametric model $q_\\phi$, we can use a Normalizing Flow for instance ;-)\n",
        "- a loss function that minimizes the distance between $p$ and $q_\\phi$\n",
        "\n",
        "\n",
        "The loss function typically used for VI is the Evidence Lower-Bound (ELBO) (the same one as is used in a VAE ;-) ). The ELBO is the right hand side part of this expression:\n",
        "\n",
        "$$ p_\\phi(y_{obs}) \\geq \\mathbb{E}_{z \\sim q_\\phi(z | y_{obs})}\\left[ \\log p(y_{obs} | z) \\right] - KL(q_\\phi || p) $$\n",
        "where $p$ in the KL divergence term is the prior $p(z)$.\n",
        "\n",
        "In other words, maximizing the ELBO tries to maximize the likelihood of the data under the model, while not going to far away from the prior $p(z)$.\n",
        "\n",
        "Because it will be more suited to our case **in practice we will use this equivalent variant of the ELBO**:\n",
        "\n",
        "$$ ELBO = \\mathbb{E}_{z \\sim q_\\phi}\\left[ \\log p(y_{obs}, z) \\right] - \\mathbb{E}_{z \\sim q_\\phi}\\left[ \\log q(z) \\right]$$\n",
        "It is more convenient for us because we can reuse our joint log posterior function defined in the previous step."
      ],
      "metadata": {
        "id": "pDEiuiAW_1VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building a model for the posterior\n",
        "\n",
        "Our first step is to build a distribution model that will act as our approximation for the posterior. For this, we will use a Normalizing Flow.\n",
        "\n",
        "We direct the interested reader to [this notebook](https://github.com/EiffL/Tutorials/blob/master/NormalizingFlowsInJAX.ipynb) to better understand how these work."
      ],
      "metadata": {
        "id": "NglqFNG2qBHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "import tqdm\n",
        "import optax\n",
        "import haiku as hk\n",
        "import flax.linen as nn\n",
        "\n",
        "# Create a random sequence\n",
        "rng_seq = hk.PRNGSequence(42)\n",
        "\n",
        "class AffineCouplingLayer(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x, nunits):\n",
        "    net = nn.leaky_relu(nn.Dense(128)(x))\n",
        "    net = nn.leaky_relu(nn.Dense(128)(net))\n",
        "    shift = nn.Dense(nunits)(net)\n",
        "    scale = nn.softplus(nn.Dense(nunits)(net)) + 1e-3 # For numerical stability\n",
        "    return  tfb.Chain([ tfb.Shift(shift), tfb.Scale(scale)])\n",
        "\n",
        "def make_nvp_fn(n_layers=5, d=32):\n",
        "  # We alternate between permutations and flow layers\n",
        "  layers = [ tfb.Permute(np.arange(d)[::-1])(tfb.RealNVP(d//2,\n",
        "                                            bijector_fn=AffineCouplingLayer(name='affine%d'%i)))\n",
        "            for i in range(n_layers) ]\n",
        "\n",
        "  # We build the actual nvp from these bijectors and a standard Gaussian distribution\n",
        "  nvp = tfd.TransformedDistribution(\n",
        "              tfd.MultivariateNormalDiag(jnp.zeros(d), jnp.ones(d)),\n",
        "              bijector=tfb.Chain(layers))\n",
        "  return nvp\n",
        "\n",
        "class NeuralFlowSampler(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self,  key, n_samples):\n",
        "    nvp = make_nvp_fn()\n",
        "    x = nvp.sample(n_samples, seed=key)\n",
        "    return x, nvp.log_prob(x)\n",
        "\n",
        "# Let's instantiate and initialize the model\n",
        "variational_posterior = NeuralFlowSampler()\n",
        "params = variational_posterior.init(jax.random.PRNGKey(42), jax.random.PRNGKey(1), 32)"
      ],
      "metadata": {
        "id": "wd_cy1NkBg3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing down the Evidence Lower Bound"
      ],
      "metadata": {
        "id": "eI9bYmm1qsu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have our posterior model, we can write the elbo as a function\n",
        "# of its parameters\n",
        "def elbo(params, seed, n_samples=16):\n",
        "  # Draw samples and their associated log probabilities from the model\n",
        "  z_q, log_q = variational_posterior.apply(params, key=seed, n_samples=n_samples)\n",
        "\n",
        "  # Write down the ELBO\n",
        "  # elbo = # EXERCISE: write down the ELBO using the formula provided at the top of this section\n",
        "\n",
        "  # we return -elbo because we will be minimizing instead of maximizing\n",
        "  return - elbo"
      ],
      "metadata": {
        "id": "O3x8deJxBg0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we define our optimizer to minimize the elbo\n",
        "\n",
        "total_steps = 10000\n",
        "lr_scheduler = optax.piecewise_constant_schedule(init_value=0.001,\n",
        "                  boundaries_and_scales={int(total_steps*0.1):0.2,\n",
        "                                         int(total_steps*0.6):0.1,})\n",
        "\n",
        "optimizer = optax.adam(learning_rate=lr_scheduler)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "@jax.jit\n",
        "def step(params, opt_state,seed):\n",
        "  loss_value, grads = jax.value_and_grad(elbo)(params,seed)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss_value\n",
        "losses = []"
      ],
      "metadata": {
        "id": "w5acIHhYBgxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm.tqdm(range(total_steps)):\n",
        "  params, opt_state, l = step(params, opt_state, next(rng_seq))\n",
        "  losses.append(l)"
      ],
      "metadata": {
        "id": "7DjL5PepBguA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(losses[100:])"
      ],
      "metadata": {
        "id": "T6gtNQiuDmSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's sample from the flow before training to see what it looks like\n",
        "z, logp = variational_posterior.apply(params, key=jax.random.PRNGKey(1), n_samples=5000)"
      ],
      "metadata": {
        "id": "pGbOe_leDrzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chainconsumer import ChainConsumer\n",
        "\n",
        "# We can take a look at the first 16 dimensions of our 32-d posterior\n",
        "c = ChainConsumer()\n",
        "c.add_chain(randn(10000, 16), name='Prior')\n",
        "c.add_chain(z[:,:16], name=\"VI Posterior\")\n",
        "fig = c.configure(usetex=False).plotter.plot()"
      ],
      "metadata": {
        "id": "2Q4zgt05Du1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# But more interestingly, let's look at our posterior solutions\n",
        "figure(figsize=[20,5])\n",
        "subplot(141)\n",
        "imshow(x_true, cmap='gray')\n",
        "title('True Hubble Image')\n",
        "\n",
        "subplot(142)\n",
        "x = generative_model(z[1])\n",
        "imshow(x, cmap='gray')\n",
        "title('Reconstructed MAP solution')\n",
        "\n",
        "subplot(143)\n",
        "imshow(x_true - x, cmap='gray')\n",
        "title('Residuals in latent space');\n",
        "\n",
        "subplot(144)\n",
        "imshow((im_noise - simulate_hsc(x, cosmos['psf'], hsc['psf'])), cmap='gray')\n",
        "title('Residuals in observation space');"
      ],
      "metadata": {
        "id": "EmoLICNQDuxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To help us visualize the posterio, we can also make a movie from posterior samples:"
      ],
      "metadata": {
        "id": "vbXrrbwVmQxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following is borrowed from https://github.com/ericjang/nf-jax/blob/master/nf-tutorial-jax.ipynb\n",
        "from matplotlib import animation, rc\n",
        "# from IPython.display import HTML, Image\n",
        "# equivalent to rcParams['animation.html'] = 'html5'\n",
        "rc('animation', html='html5')"
      ],
      "metadata": {
        "id": "6ZSgIFQCH2Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2, ax3, ax4) = subplots(1, 4, figsize=[20,5])\n",
        "ax1.imshow(x_true, cmap='gray')\n",
        "ax1.set_title('True Hubble Image')\n",
        "\n",
        "x = generative_model(z[0])\n",
        "im2 = ax2.imshow(x, vmax=x_true.max(), cmap='gray')\n",
        "ax2.set_title('Posterior sample')\n",
        "\n",
        "im3 = ax3.imshow(x_true - x, cmap='gray')\n",
        "ax3.set_title('Residuals of solution');\n",
        "\n",
        "im4 = ax4.imshow((y_obs - simulate_hsc(x, cosmos['psf'], hsc['psf'])), cmap='gray')\n",
        "ax4.set_title('Residuals on observations');"
      ],
      "metadata": {
        "id": "__aMNCSMIcGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def animate(i):\n",
        "  x = generative_model(z[i+1])\n",
        "  im2.set_data(x)\n",
        "  im3.set_data(x_true - x)\n",
        "  im4.set_data((y_obs - simulate_hsc(x, cosmos['psf'], hsc['psf'])))\n",
        "  return (im2, im3, im4)"
      ],
      "metadata": {
        "id": "j9dtmrxvImrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anim = animation.FuncAnimation(fig, animate, frames=100, interval=100, blit=False)"
      ],
      "metadata": {
        "id": "lfnwLH4TI2Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anim"
      ],
      "metadata": {
        "id": "Ti73nMw7IryP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we can for instance look at the mean and standard deviation of these solutions\n",
        "xs = jax.vmap(generative_model)(z[:256])"
      ],
      "metadata": {
        "id": "SJtEL7BQbVfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=[15,5])\n",
        "subplot(131)\n",
        "imshow(x_true, cmap='gray'); axis('off')\n",
        "title('truth')\n",
        "subplot(132)\n",
        "imshow(xs.mean(axis=0),vmax=x_true.max(), cmap='gray'); axis('off')\n",
        "title('mean posterior')\n",
        "subplot(133)\n",
        "imshow(xs.std(axis=0), cmap='gray'); axis('off')\n",
        "title('standard deviation')"
      ],
      "metadata": {
        "id": "7GcAhcrMbXEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You now know how to use a generative model to solve an inverse\n",
        "problem!"
      ],
      "metadata": {
        "id": "DKODrsqsrDWt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzH1TfMQhv40"
      },
      "source": [
        "## [Optional]: Naive Deep Learning  Approach\n",
        "\n",
        "Now that we have covered the basics of how to manipulate the problem, we can try a Deep Learning approach. Can a CNN recover the space-based image?\n",
        "\n",
        "It should be noted that this is really not the approach we will try to follow in this data challenge, but this is a good opportunity to learn how to write and train a neural network in JAX.\n",
        "\n",
        "We will use the [Haiku](https://github.com/deepmind/dm-haiku) library from DeepMind for all neural network related things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jOvG7bIhv40"
      },
      "source": [
        "#### Building data pipeline\n",
        "\n",
        "Let's start with a pipeline creating examples of HST and HSC images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPw9Avjphv40"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def preprocess(example):\n",
        "    \"\"\" Augment COSMOS dataset with random flips\n",
        "    \"\"\"\n",
        "    x = example['image'][...,tf.newaxis]\n",
        "    x = tf.image.flip_left_right(x)\n",
        "    x = tf.image.flip_up_down(x)\n",
        "    return {'image':x[...,0], 'psf':example['psf']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRj_F88fhv40"
      },
      "outputs": [],
      "source": [
        "# Load COSMOS\n",
        "dset_cosmos = tfds.load(\"Cosmos/23.5\", split=tfds.Split.TRAIN,\n",
        "                        data_dir='galsim/tensorflow_datasets')\n",
        "dset_cosmos = dset_cosmos.cache()\n",
        "dset_cosmos = dset_cosmos.repeat()\n",
        "dset_cosmos = dset_cosmos.map(preprocess)\n",
        "dset_cosmos = dset_cosmos.shuffle(40000)\n",
        "\n",
        "# Load HSC\n",
        "dset_hsc = tfds.load(\"HSC\", split=tfds.Split.TRAIN,\n",
        "                        data_dir='galsim/tensorflow_datasets')\n",
        "dset_hsc = dset_hsc.cache()\n",
        "dset_hsc = dset_hsc.repeat()\n",
        "dset_hsc = dset_hsc.shuffle(10000)\n",
        "\n",
        "# Combine both datasets\n",
        "combined_dset = tf.data.Dataset.zip((dset_cosmos, dset_hsc))\n",
        "combined_dset = combined_dset.batch(64)                      # Adds batching\n",
        "combined_dset = combined_dset.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg0MboRJhv40"
      },
      "outputs": [],
      "source": [
        "batch = next(combined_dset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7YM4wVYhv40"
      },
      "outputs": [],
      "source": [
        "# A batch contains examples from both datasets\n",
        "cosmos, hsc = batch\n",
        "\n",
        "figure(figsize=[10,5])\n",
        "subplot(121)\n",
        "imshow(cosmos['image'][0],cmap='gray')\n",
        "title('Galaxy')\n",
        "subplot(122)\n",
        "imshow(cosmos['psf'][0],cmap='gray')\n",
        "title('PSF');\n",
        "\n",
        "figure(figsize=[20,5])\n",
        "subplot(141)\n",
        "imshow(hsc['image'][0],cmap='gray')\n",
        "title('Galaxy')\n",
        "subplot(142)\n",
        "imshow(hsc['psf'][0],cmap='gray')\n",
        "title('PSF')\n",
        "subplot(143)\n",
        "imshow(hsc['mask'][0] == 44,cmap='gray')\n",
        "title('Interpolated pixels')\n",
        "subplot(144)\n",
        "imshow(hsc['variance'][0],cmap='gray')\n",
        "title('Variance plane');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIpNcEBzhv40"
      },
      "source": [
        "#### Defining a Neural Network\n",
        "\n",
        "For this kind of thing, one may think some kind of Unet might be able to solve this deconvolution task.\n",
        "\n",
        "In Haiku, Neural Networks are defined by creating a `hk.Module` subclass and populating its `__call__` member"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm42qnuJhv40"
      },
      "outputs": [],
      "source": [
        "import haiku as hk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUHnVzEzhv40"
      },
      "outputs": [],
      "source": [
        "class Unet(hk.Module):\n",
        "    \"\"\" The most trivial Unet possible\n",
        "    \"\"\"\n",
        "    def __call__(self, x):\n",
        "        x = hk.Conv2D(16, kernel_shape=3)(x)\n",
        "        l1 = jax.nn.leaky_relu(x)\n",
        "        x = hk.avg_pool(l1, window_shape=3, strides=2, padding='SAME')\n",
        "\n",
        "        x = hk.Conv2D(32, kernel_shape=3)(x)\n",
        "        l2 = jax.nn.leaky_relu(x)\n",
        "        x = hk.avg_pool(l2,  window_shape=3, strides=2, padding='SAME')\n",
        "\n",
        "        x = hk.Conv2D(64, kernel_shape=3)(x)\n",
        "        l3 = jax.nn.leaky_relu(x)\n",
        "        x = hk.avg_pool(l3, window_shape=3, strides=2, padding='SAME')\n",
        "\n",
        "        x = hk.Conv2D(128, kernel_shape=3)(x)\n",
        "        x = jax.nn.leaky_relu(x)\n",
        "\n",
        "        x = hk.Conv2DTranspose(64, kernel_shape=3, stride=2)(x)\n",
        "        x = jnp.concatenate([x, l3], axis=-1)\n",
        "\n",
        "        x = jax.nn.leaky_relu(x)\n",
        "        x = hk.Conv2DTranspose(32, kernel_shape=3, stride=2)(x)\n",
        "        x = jnp.concatenate([x, l2], axis=-1)\n",
        "\n",
        "        x = jax.nn.leaky_relu(x)\n",
        "        x = hk.Conv2DTranspose(16, kernel_shape=5, stride=2)(x)\n",
        "        x = jnp.concatenate([x, l1], axis=-1)\n",
        "\n",
        "        x = hk.Conv2D(1, kernel_shape=5)(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPKuTKehhv40"
      },
      "outputs": [],
      "source": [
        "# Transform model into pure functions\n",
        "model = hk.without_apply_rng(hk.transform(lambda x: Unet()(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LE_uU1khv40"
      },
      "outputs": [],
      "source": [
        "# Initialize model parameters\n",
        "params = model.init(jax.random.PRNGKey(0), jnp.zeros([1,128,128,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdR2oXOshv41"
      },
      "outputs": [],
      "source": [
        "def loss_fn(params, rng, batch, return_images=False):\n",
        "    # Create mock observations\n",
        "    cosmos, hsc = batch\n",
        "\n",
        "    def prep_data(cosmos, hsc):\n",
        "        \"\"\" Prepares data for the neural network\n",
        "        \"\"\"\n",
        "        obs = tfd.Independent(tfd.Normal(jax.vmap(simulate_hsc)(cosmos['image'], cosmos['psf'], hsc['psf']),\n",
        "                         jnp.sqrt(hsc['variance'])),\n",
        "                             reinterpreted_batch_ndims=2).sample(seed=rng)\n",
        "\n",
        "        # Rescale obs to same pixel scale to make the life of the network easier\n",
        "        x = jax.image.scale_and_translate(obs, [len(obs), 128, 128], [1,2],\n",
        "                                            jnp.array([5.6, 5.6]),\n",
        "                                            jnp.array([128/2 - 41/2*5.6 ,128/2 - 41/2*5.6 ]),\n",
        "                                            jax.image.ResizeMethod.CUBIC)\n",
        "\n",
        "        # Resize input and outputs for the network\n",
        "        im  = jnp.pad(cosmos['image'], [[0,0], [14,13] , [14,13]])\n",
        "\n",
        "        # Adding channel dimension\n",
        "        im = jnp.expand_dims(im, -1)\n",
        "        x = jnp.expand_dims(x, -1)\n",
        "        return im, x, obs\n",
        "\n",
        "    im, x, obs = prep_data(cosmos, hsc )\n",
        "\n",
        "    # Apply neural network\n",
        "    rec = model.apply(params, x)\n",
        "\n",
        "    loss = jnp.mean( jnp.sum( (rec - im)**2, axis=[1, 2, 3]))\n",
        "\n",
        "    if return_images:\n",
        "        return im, obs, x, rec, loss\n",
        "    else:\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS2tc3ZOhv41"
      },
      "outputs": [],
      "source": [
        "optimizer = optax.adam(0.001)\n",
        "opt_state = optimizer.init(params)\n",
        "rng_seq = hk.PRNGSequence(12)\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NBYKeb6hv41"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def update(params, rng, opt_state):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, rng, batch)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    # Apply gradient descent\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return loss, params, opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj2kQQLghv41"
      },
      "outputs": [],
      "source": [
        "for i in range(10000):\n",
        "    batch = next(combined_dset)\n",
        "    loss, params, opt_state = update(params, next(rng_seq), opt_state)\n",
        "    losses.append(loss)\n",
        "    if i %100 ==0:\n",
        "        print('step',i,loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP4ndaaJhv41"
      },
      "outputs": [],
      "source": [
        "loglog(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxuXh9nthv41"
      },
      "outputs": [],
      "source": [
        "# Create a test dataset\n",
        "dset_cosmos_test = tfds.load(\"Cosmos/23.5\",\n",
        "                        split=tfds.Split.TEST)\n",
        "dset_cosmos_test = dset_cosmos_test.cache()\n",
        "dset_cosmos_test = dset_cosmos_test.repeat()\n",
        "\n",
        "dset_hsc = tfds.load(\"HSC\",\n",
        "                     split=tfds.Split.TRAIN)\n",
        "dset_hsc = dset_hsc.cache()\n",
        "dset_hsc = dset_hsc.repeat()\n",
        "dset_hsc = dset_hsc.shuffle(10000)\n",
        "\n",
        "combined_dset_test = dset_hsc.zip((dset_cosmos_test , dset_hsc))\n",
        "combined_dset_test = combined_dset_test.batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRNlZ75Ghv41"
      },
      "outputs": [],
      "source": [
        "it_test  = combined_dset_test.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE8LFSWzhv41"
      },
      "outputs": [],
      "source": [
        "batch = next(it_test)\n",
        "im, obs, y, rec, loss = loss_fn(params, next(rng_seq), batch, return_images=True)\n",
        "cosmos, hsc = batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "8QUvQNMshv41"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    figure(figsize=[20,5])\n",
        "    subplot(141)\n",
        "    imshow(im[i])\n",
        "    subplot(142)\n",
        "    imshow(obs[i])\n",
        "    subplot(143)\n",
        "    imshow(rec[i])\n",
        "    subplot(144)\n",
        "    # Removing the padding\n",
        "    x = rec[i][14:-13:,14:-13:,0]\n",
        "    x = simulate_hsc(x, cosmos['psf'][i], hsc['psf'][i])\n",
        "    # Show residuals on observations\n",
        "    imshow(obs[i] - x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkI5QiBNhv41"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WzH1TfMQhv40",
        "_jOvG7bIhv40",
        "FIpNcEBzhv40"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}