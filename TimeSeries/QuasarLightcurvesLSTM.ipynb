{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANF_DL_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUhoOLQ5wNBUPx3H9OZxXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EiffL/Tutorials/blob/master/TimeSeries/QuasarLightcurvesLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dI3Bo0-pqe"
      },
      "source": [
        "##### Copyright 2017-2021 Francois Lanusse.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yq78g3O-pk5"
      },
      "source": [
        "# Quasar classification by LSTM\n",
        "\n",
        "\n",
        "Author: [@EiffL](https://github.com/EiffL) (Francois Lanusse)\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this notebook, we are going to use LSTMs to classify between stars and quasars, using only information about the flux at different point in time.\n",
        "\n",
        "The idea is to distinguish between stars, and quasars, based on how their fluxes change with time.\n",
        "\n",
        "We are using real light curve data from the SDSS survey, and using an catalog of known quasars for our training set. \n",
        "\n",
        "### Learning objectives:\n",
        "In this notebook, we will learn how to:\n",
        "\n",
        "  - Learn how to handle time-series data of different lenghts\n",
        "  - Use Keras for building an LSTM\n",
        "\n",
        "\n",
        "### Instructions for enabling GPU access\n",
        "\n",
        "By default, notebooks are started without acceleration. To make sure that the runtime is configured for using GPUs, go to `Runtime > Change runtime type`, and select GPU in `Hardware Accelerator`.\n",
        "\n",
        "\n",
        "\n",
        "### Installs and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgb70M3RBFBQ"
      },
      "source": [
        "%pylab inline\n",
        "import tensorflow as tf\n",
        "from astropy.table import Table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSg0_CdcSh_E"
      },
      "source": [
        "### Checking for GPU access\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad0cLFgJSoNc"
      },
      "source": [
        "#Checking for GPU access\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEtrGZmL-pVg"
      },
      "source": [
        "## Retrieving data\n",
        "\n",
        "In this first section, we retrieve a dataset of lightcurves as generated by this [script](https://github.com/McWilliamsCenter/CMUCosmoML/tree/master/applications/quasar_classification) by running an SQL query on the SDSS servers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLqP7OwCTMnQ"
      },
      "source": [
        "# Google Cloud Storage bucket for Estimator logs and storing\n",
        "# the training dataset.\n",
        "bucket = 'ahw2019' # Bucket setup for this AHW2019 tutorial\n",
        "print('Using bucket: {}'.format(bucket))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kTQjCjGTUFm"
      },
      "source": [
        "!gsutil -m cp gs://{bucket}/quasar/ligthcurve_data.fits.gz ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZQaE6duTMQz"
      },
      "source": [
        "This will download locally the training set. We can now load it to build our input pipeline.\n",
        "\n",
        "\n",
        "Below we create functions that can build input pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUjI9tX1_4cE"
      },
      "source": [
        "# Loading dataset\n",
        "data_table = Table.read('ligthcurve_data.fits.gz')\n",
        "\n",
        "# Splitting training and testing data\n",
        "randomize_inds = range(len(data_table))\n",
        "randomize_inds = permutation(randomize_inds)\n",
        "randomized_inds_train = randomize_inds[0:45000]\n",
        "randomized_inds_test  = randomize_inds[45000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vSe-ceGhlNd"
      },
      "source": [
        "`data_table` contains all of the data available to us from the database:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVzE6mLjhyZ6"
      },
      "source": [
        "data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXbZlLrsh-8I"
      },
      "source": [
        "In particular, it has a fixed size `time_series` field and an `obs_len` field. This `obs_len` tells us how many points we actually have in our timeseries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2qJhy2-iXKZ"
      },
      "source": [
        "# Let's plot the observation of the flux of some quasars/stars in different\n",
        "# filters:\n",
        "plot(data_table['time_series'][0][:,1], '+')\n",
        "plot(data_table['time_series'][0][:,2], '+')\n",
        "plot(data_table['time_series'][0][:,3], '+')\n",
        "plot(data_table['time_series'][0][:,4], '+')\n",
        "axvline(data_table['obs_len'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26oXtguAi08u"
      },
      "source": [
        "Let's have a look at a different entry:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlJ-1HKdiyxO"
      },
      "source": [
        "plot(data_table['time_series'][1][:,1], '+')\n",
        "plot(data_table['time_series'][1][:,2], '+')\n",
        "plot(data_table['time_series'][1][:,3], '+')\n",
        "plot(data_table['time_series'][1][:,4], '+')\n",
        "axvline(data_table['obs_len'][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDJ6doRzjGh4"
      },
      "source": [
        "They have different lengths, how do we provide this information to the RNN? \n",
        "\n",
        "We can use a `masking` mechanism. We always send arrays of the same size, but some timesteps, if they are not actually observed are set to a specific value, i.e. -99 and the network will skip these missing steps.\n",
        "\n",
        "Let's create a pre-processing function that can format our data on this principle:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHUlIp1zcHUb"
      },
      "source": [
        "def mapping_function(x):\n",
        "    def extract_batch(inds):\n",
        "        inds = randomized_inds_train[inds]\n",
        "        ts = clip(data_table['time_series'][inds].astype('float32'),-10,10) \n",
        "        length = clip(data_table['obs_len'][inds],0,89).astype('int32')\n",
        "        ts[length:,:] = -99. # Any points in the light curve after obs_len is set to -99\n",
        "        return data_table['coadd_label'][inds].astype('float32'), ts\n",
        "    a,b =tf.py_function( extract_batch, [x], [tf.float32, tf.float32])\n",
        "    a.set_shape([])\n",
        "    b.set_shape([90,12])\n",
        "    return a,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0XB2wPcQEe"
      },
      "source": [
        "# And we can apply this pre-processing function on our data to build a tf.data.Dataset\n",
        "dataset = tf.data.Dataset.range(len(randomized_inds_train))\n",
        "dataset = dataset.map(mapping_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oTWyPprcQ4u"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZJ5rnzBciB8"
      },
      "source": [
        "# Let's grab an example\n",
        "for batch in dataset.take(1):\n",
        "  plot(batch[1][:,1], '+')\n",
        "  plot(batch[1][:,2], '+')\n",
        "  plot(batch[1][:,3], '+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHeDgvkkHC2"
      },
      "source": [
        "Ok great, now we are going to create functions that can produce datasets for various training and testing scenarios.\n",
        "\n",
        "In the functions below, what changes is which dataset we are using (training or testing), and whether or not entries are shuffled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmjPHqbJ-dfk"
      },
      "source": [
        "# Define input function for training \n",
        "def input_fn_train():\n",
        "  def mapping_function(x):\n",
        "      def extract_batch(inds):\n",
        "          inds = randomized_inds_train[inds]\n",
        "          ts = clip(data_table['time_series'][inds].astype('float32'),-10,10) \n",
        "          length = clip(data_table['obs_len'][inds],0,89).astype('int32')\n",
        "          ts[length:,:] = -99. \n",
        "          return data_table['coadd_label'][inds].astype('float32'), ts\n",
        "      a,b =tf.py_function( extract_batch, [x], [tf.float32, tf.float32])\n",
        "      a.set_shape([]) # This is the label\n",
        "      b.set_shape([90,12]) # This is the input light curve\n",
        "      return b,a\n",
        "\n",
        "  dataset = tf.data.Dataset.range(len(randomized_inds_train))\n",
        "  dataset = dataset.map(mapping_function)\n",
        "  dataset = dataset.cache()\n",
        "  dataset = dataset.repeat().shuffle(20000).batch(256)\n",
        "  return  dataset\n",
        "\n",
        "# Define input function for testing on the training set\n",
        "def input_fn_train_test():\n",
        "  def mapping_function(x):\n",
        "      def extract_batch(inds):\n",
        "          inds = randomized_inds_train[inds]\n",
        "          ts = clip(data_table['time_series'][inds].astype('float32'),-10,10) \n",
        "          length = clip(data_table['obs_len'][inds],0,89).astype('int32')\n",
        "          ts[length:,:] = -99. \n",
        "          return data_table['coadd_label'][inds].astype('float32'), ts\n",
        "      a,b =tf.py_function( extract_batch, [x], [tf.float32, tf.float32])\n",
        "      a.set_shape([]) # This is the label\n",
        "      b.set_shape([90,12]) # This is the input light curve\n",
        "      return b,a\n",
        "\n",
        "  dataset = tf.data.Dataset.range(len(randomized_inds_train))\n",
        "  dataset = dataset.map(mapping_function)\n",
        "  dataset = dataset.batch(256)\n",
        "  return  dataset\n",
        "\n",
        "# Define input function for testing on the testing set\n",
        "def input_fn_test():\n",
        "  def mapping_function(x):\n",
        "      def extract_batch(inds):\n",
        "          inds = randomized_inds_test[inds]\n",
        "          ts = clip(data_table['time_series'][inds].astype('float32'),-10,10) \n",
        "          length = clip(data_table['obs_len'][inds],0,89).astype('int32')\n",
        "          ts[length:,:] = -99. \n",
        "          return data_table['coadd_label'][inds].astype('float32'), ts\n",
        "      a,b =tf.py_function( extract_batch, [x], [tf.float32, tf.float32])\n",
        "      a.set_shape([]) # This is the label\n",
        "      b.set_shape([90,12]) # This is the input light curve\n",
        "      return b,a\n",
        "\n",
        "  dataset = tf.data.Dataset.range(len(randomized_inds_test))\n",
        "  dataset = dataset.map(mapping_function)\n",
        "  dataset = dataset.batch(256)\n",
        "  return  dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tDPLvEx_9Zf"
      },
      "source": [
        "## Building the Neural Network\n",
        "\n",
        "Now that we have the tools to load the data, the next step is to build the LSTM model. We will start with the simplest model possible, an LSTM layer that will process the time-series, followed by a Dense layer that will produce a probability of a timeseries to be a quasar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0_-wkwl_8Ge"
      },
      "source": [
        "tfkl = tf.keras.layers\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tfkl.InputLayer([90,12]),\n",
        "  tfkl.Masking(mask_value=-99.), # This is to tell Keras to skip the missing time steps\n",
        "\n",
        "  # Create your LSTM model :-) \n",
        "  # Here are a few hints:\n",
        "  # - You probably want to have one LSTM layer, followed by one or more Dense layers\n",
        "  # - You probably want to use tfkl.LSTM (https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
        "  # - The output should be a detection probabilty, so between 0 and 1\n",
        "  # .....\n",
        "\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXwg2PZlBB-c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2oSjl9sdlSd"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss= # What loss function should we use?\n",
        "              ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH5bb_x7B0p3"
      },
      "source": [
        "# Let's build the input dataset\n",
        "dataset_training = input_fn_train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFATBIiodhGB"
      },
      "source": [
        "# And fit the model\n",
        "model.fit(dataset_training, \n",
        "          steps_per_epoch=45000//256,\n",
        "          epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DENQHfNsCLK1"
      },
      "source": [
        "### Applying the neural network on the testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfp9HByfCuiX"
      },
      "source": [
        "# Evaluating performance on testing set\n",
        "dataset_testing = input_fn_test()\n",
        "test_prob = np.concatenate([model(batch[0]) for batch in dataset_testing])\n",
        "\n",
        "# Concatenating the predicted probabilities\n",
        "table_test = data_table[randomized_inds_test]\n",
        "table_test['p'] = test_prob.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npKy4ULgf_l3"
      },
      "source": [
        "# Evaluating performance on training set\n",
        "dataset_training = input_fn_train_test()\n",
        "train_prob = np.concatenate([model(batch[0]) for batch in dataset_training])\n",
        "\n",
        "# Concatenating the predicted probabilities\n",
        "table_train = data_table[randomized_inds_train]\n",
        "table_train['p'] = train_prob.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1d4de7WDNkw"
      },
      "source": [
        "# Compute ROC curves \n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr1, tpr1, thr1 = roc_curve(table_train['coadd_label'], table_train['p'])\n",
        "fpr2, tpr2, thr2 = roc_curve(table_test['coadd_label'],  table_test['p'])\n",
        "\n",
        "plot(fpr1, tpr1,label='training set')\n",
        "plot(fpr2, tpr2,label='testing set')\n",
        "grid('on')\n",
        "xscale('log')\n",
        "legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Tc40efPfWU"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(\"Training Set ROC AUC score:\", roc_auc_score(table_train['coadd_label'], table_train['p']))\n",
        "print(\"Test Set ROC AUC score:\", roc_auc_score(table_test['coadd_label'], table_test['p']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_s7CfcatJpB"
      },
      "source": [
        "And here  we go! A near perfect Quasar detector :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkZYkk7SmGWB"
      },
      "source": [
        "## Going Further\n",
        "\n",
        "This is pretty much all you need to know to implement a Recurrent Neural Network. You can find the reference guide for the TensorFlow Keras guide [here](https://www.tensorflow.org/guide/keras/rnn).\n",
        "\n",
        "To go further in this tutorial, you can do the following things:\n",
        "\n",
        "  - LSTMs are **notorious for overfitting** data very easily, to prevent this, you can add a `dropout` value to the `LSTM` layer.\n",
        "  - You can replace the `LSTM` layer by a different RNN type, for instance [GRU](https://keras.io/api/layers/recurrent_layers/gru/).\n",
        "  - You can add a second LSTM layer, like so:\n",
        "```\n",
        "...\n",
        "  tfkl.LSTM(128, return_sequences=True),\n",
        "  tfkl.LSTM(128),\n",
        "...\n",
        "```\n",
        "\n"
      ]
    }
  ]
}