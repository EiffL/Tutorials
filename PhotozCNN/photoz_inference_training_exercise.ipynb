{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "photoz_inference_training_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EiffL/Tutorials/blob/master/PhotozCNN/photoz_inference_training_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNYxkv19Bb3u"
      },
      "source": [
        "##### Copyright 2019-2021 Francois Lanusse.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfcfTuBaBg5l"
      },
      "source": [
        "# Photometric Redshift Estimation with TensorFlow - Part III: Training a CNN with Keras\n",
        "\n",
        "Author: [@EiffL](https://github.com/EiffL) (Francois Lanusse)\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this tutorial, we learn how to combine Keras, TensorFlow Probability, and Google Colab to train a model for photo-z inference in the cloud.\n",
        "\n",
        "We will be using data from the HSC Survey, and more specifically from the Public Data Release 2, which can be found here: https://hsc-release.mtk.nao.ac.jp/doc/\n",
        "\n",
        "\n",
        "The dataset contains postage stamps of galaxies in 5 HSC bands, along with corresponding spectroscopic redshifts.\n",
        "\n",
        "Our goal will be to estimate redshift just by looking at a picture of a galaxy.\n",
        "\n",
        "### Learning objectives\n",
        "\n",
        "In this notebook, we will learn how to:\n",
        "*   Build a tf.data.Dataset input pipeline.\n",
        "*   Build a simple convolutional neural network with Keras.\n",
        "*   Train a model on GPUs in the cloud.\n",
        "*   (Stretch Goal) Use TensorFlow Probability to make a probabilistic model.\n",
        "\n",
        "Note: this Tutorial was originaly presented at [Astro Hack Week 2019](https://github.com/AstroHackWeek/AstroHackWeek2019/tree/master/day4_bayesiandeep).\n",
        "\n",
        "### Instructions for enabling GPU access\n",
        "\n",
        "By default, notebooks are started without acceleration. To make sure that the runtime is configured for using GPUs, go to `Runtime > Change runtime type`, and select GPU in `Hardware Accelerator`.\n",
        "\n",
        "\n",
        "\n",
        "### Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuePydw28htG"
      },
      "source": [
        "# Retrieving utilities\n",
        "!git clone https://github.com/EiffL/Tutorials.git\n",
        "%cd Tutorials/PhotozCNN\n",
        "\n",
        "# Retrieving pre-prepared data, it takes 2 minutes.\n",
        "!gsutil -m -q cp -r gs://ahw2019/hsc_photoz/tensorflow_datasets /root/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGBGfUdSF7QH"
      },
      "source": [
        "### Checking for GPU access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgs_ONXTF6vX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#Checking for GPU access\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HRAxSC_JS3R"
      },
      "source": [
        "## Building a tf.data.Dataset Input Pipeline\n",
        "\n",
        "The first step is to read the data from disk into a tf.data.Dataset. This TensorFlow Dataset API is the canonical way to supply data to a model during training. It is fast and optimized, and supports distributed training!\n",
        "\n",
        "Converting some input data into a tf.data.Dataset is made completely trivial by the [TensorFlow Datasets library](https://www.tensorflow.org/datasets) that we explored in the [previous notebook](https://github.com/EiffL/Tutorials/blob/master/PhotozCNN/photoz_inference_tfdatasets.ipynb) in this series. \n",
        "We will be reusing the dataset created there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_BYy7UQ85BP"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import hsc_photoz\n",
        "\n",
        "train_dset = tfds.load('hsc_photoz', split='train')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHcy8OgQTQMW"
      },
      "source": [
        "# Let's extract one example of our data\n",
        "for i in train_dset.take(1):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVBQKpT7EhDa"
      },
      "source": [
        "Nice :-) it's working, we had nothing to do. We did cheat a bit though, the pre-built dataset is already cached in this instance. On your own computer it might take about one hour to download and prepare the full dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YYlkH3dLm_2"
      },
      "source": [
        "### Dataset preprocessing\n",
        "\n",
        "An important step of any input pipeline is to make\n",
        "sure the data is reasonably well behaved before \n",
        "feeding to the neural network. Here are some common strategies:\n",
        "\n",
        "\n",
        "*   Apply log() to values with large dynamic range\n",
        "*   Remove means, and standardize standard deviation\n",
        "*   etc...\n",
        "\n",
        "\n",
        "So, we begin by looking at our data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHIKuq5OMa7E"
      },
      "source": [
        "# What's in our dataset:\n",
        "train_dset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PRSqauJTt5f"
      },
      "source": [
        "Ok, we see that this dataset is a dictionary, field `image` are hsc cutouts in 5 bands (g,r,i,z,y), this will be the inputs to our CNN. We also see a `specz_redshift` entry under `attrs`, that will be our prediction target. Let's have a look at these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm0ifpc4VrYc"
      },
      "source": [
        "from astropy.visualization import make_lupton_rgb\n",
        "%pylab inline \n",
        "\n",
        "# The data is in 5 bands GRIZY, but for visualisation we use only the\n",
        "# 3 first bands and luptonize them\n",
        "def luptonize(img):\n",
        "  return make_lupton_rgb(img[:,:,2], img[:,:,1], img[:,:,0],\n",
        "                         Q=15, stretch=0.5, minimum=0)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i, entry in enumerate(train_dset.take(25)):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.imshow(luptonize(entry['image']))\n",
        "  plt.title('z = %0.02f'%entry['attrs']['specz_redshift'])\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htYm8k4pEzpM"
      },
      "source": [
        "How nice is that :-) We can extract postage stamps and the corresponding spectroscopic redshift for these objects. \n",
        "\n",
        "Before doing anything else, we should take a closer look at the  data and check that it's well behaved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-c7o1B4Ma4X"
      },
      "source": [
        "# Let's collect a few examples to check their distributions\n",
        "cutouts=[]\n",
        "specz = []\n",
        "for (batch, entry) in enumerate(train_dset.take(1000)):\n",
        "  specz.append(entry['attrs']['specz_redshift'])\n",
        "  cutouts.append(entry['image'])\n",
        "\n",
        "cutouts = np.stack(cutouts)\n",
        "specz = np.stack(specz)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBxLT8tfaRBK"
      },
      "source": [
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.hist(cutouts[...,i].flatten(),100, label=b);\n",
        "plt.legend()\n",
        "\n",
        "# Problem ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXkCgSHc2gBR"
      },
      "source": [
        "Do you see a problem in this histogram?\n",
        "\n",
        "Let's have a look at a few images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECvEJL_UlZ2Q"
      },
      "source": [
        "plt.figure(figsize=(15,3))\n",
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.subplot(1,5,i+1)\n",
        "  plt.imshow(cutouts[0,:,:,i],vmin=-1,vmax=50)\n",
        "  plt.title(b)\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.figure(figsize=(15,3))\n",
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.subplot(1,5,i+1)\n",
        "  plt.imshow(cutouts[1,:,:,i],vmin=-1,vmax=50)\n",
        "  plt.title(b)\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGZbzaOQ2w_m"
      },
      "source": [
        "In the plot above, each row is a different galaxy, and each column is a different band."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxhBE7xAbGpM"
      },
      "source": [
        "# Let's look at it in log scale\n",
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.hist(cutouts[...,i].flatten(),100, label=b,alpha=0.5);\n",
        "plt.legend()\n",
        "plt.yscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XwYxVLfbpnK"
      },
      "source": [
        "This is terrible, the tail of this distribution in pixel intensity is going to kill our neural networks. We need to standardize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWu_s273cqUL"
      },
      "source": [
        "# Let's evaluate the noise standard deviation in each band, and apply range \n",
        "# compression accordingly\n",
        "from astropy.stats import mad_std\n",
        "scaling = []\n",
        "\n",
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.hist(cutouts[...,i].flatten(),100, label=b,alpha=0.5,range=[-1,1]);\n",
        "  sigma = mad_std(cutouts[...,i].flatten())\n",
        "  scaling.append(sigma)\n",
        "  plt.axvline(sigma, color='C%d'%i,alpha=0.5)\n",
        "  plt.axvline(-sigma, color='C%d'%i,alpha=0.5)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asq6mhLeWAK"
      },
      "source": [
        "# Let's have a look at this distribution if we rescale each band by the standard\n",
        "# deviation\n",
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.hist(cutouts[...,i].flatten()/scaling[i],100, label=b,alpha=0.5,\n",
        "           range=[-10,10]);\n",
        "legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzE7J3AMgaj1"
      },
      "source": [
        "Sweet! Now there is still an unsigthly tail towards very large values. We are going to apply range compression to get rid of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJCS-zMsboOD"
      },
      "source": [
        "# a common approach for range compression is to apply arcsinh to suppress the\n",
        "# high amplitude values\n",
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.hist(np.arcsinh(cutouts[...,i].flatten()/scaling[i]/3),100,\n",
        "           label=b, alpha=0.5);\n",
        "plt.legend()\n",
        "plt.yscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrSqabqBgtS4"
      },
      "source": [
        "![Perfection](https://i.kym-cdn.com/entries/icons/original/000/022/900/704.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb_TCsVDfMzH"
      },
      "source": [
        "# we can have a look at individual postage stamps with or without this scaling\n",
        "subplot(121)\n",
        "imshow(cutouts[0,:,:,1]/scaling[1])\n",
        "title('Before')\n",
        "subplot(122)\n",
        "imshow(np.arcsinh(cutouts[0,:,:,1]/scaling[1]/3))\n",
        "title('After');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Az8G5YhtXj"
      },
      "source": [
        "# Let's just check the specz values\n",
        "plt.hist(specz,100);\n",
        "# Should be ok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azewX3L8F4fH"
      },
      "source": [
        "Now that we have defined a scaling for the data that should be appropriate, we can build a scaling function and apply it to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N64jVDeFhtdm"
      },
      "source": [
        "# Using a mapping function to apply preprocessing to our data\n",
        "def preprocessing(example):\n",
        "  def range_compression(img):\n",
        "    return tf.math.asinh(img / tf.constant(scaling) / 3. )\n",
        "  # Our preprocessing function only returns the postage stamps, and the specz\n",
        "  return range_compression(example['image']), example['attrs']['specz_redshift']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PigT6tEhtga"
      },
      "source": [
        "dset = train_dset.map(preprocessing)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7k9_qgzhtjk"
      },
      "source": [
        "# Let's draw some examples from this now\n",
        "cutouts=[]\n",
        "specz = []\n",
        "for (batch, entry) in enumerate(dset.take(1000)):\n",
        "  specz.append(entry[1])\n",
        "  cutouts.append(entry[0])\n",
        "\n",
        "cutouts = np.stack(cutouts)\n",
        "specz = np.stack(specz)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3myN4MnkW8Z"
      },
      "source": [
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.hist(cutouts[...,i].flatten(),100, label=b,alpha=0.5, range=[-1,6]);\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgu9NKgnMajT"
      },
      "source": [
        "plt.figure(figsize=(15,3))\n",
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.subplot(1,5,i+1)\n",
        "  plt.imshow(cutouts[0,:,:,i],vmin=-1,vmax=6)\n",
        "  plt.title(b)\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.figure(figsize=(15,3))\n",
        "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
        "  plt.subplot(1,5,i+1)\n",
        "  plt.imshow(cutouts[1,:,:,i],vmin=-1,vmax=6)\n",
        "  plt.title(b)\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXJVuvWmkZF7"
      },
      "source": [
        "Sweeeeeet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3GYqhbkl9wQ"
      },
      "source": [
        "### Create the input pipeline\n",
        "\n",
        "Now that we know how to preprocess the data, we can build the input pipeline. Below is a function that creates a Dataset object from the tfrecords files, decode them, applies preprocessing, shuffles the dataset, and create batches of data. Finally the function returns the dataset, that Keras models can directly ingest.\n",
        "\n",
        "More information about tf.data.dataset API can be found here: \n",
        "\n",
        "https://www.tensorflow.org/guide/datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59EM2vNXLKxh"
      },
      "source": [
        "# Using a mapping function to apply preprocessing to our data\n",
        "def preprocessing(example):\n",
        "  img = tf.math.asinh(example['image'] / tf.constant(scaling) / 3. )\n",
        "  return img, example['attrs']['specz_redshift']\n",
        "\n",
        "def input_fn(mode='train', batch_size=64):\n",
        "  \"\"\"\n",
        "  mode: 'train' or 'test'\n",
        "  \"\"\"\n",
        "  if mode == 'train':\n",
        "    dataset = tfds.load('hsc_photoz', split='train[:80%]')\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  else:\n",
        "    dataset = tfds.load('hsc_photoz', split='train[80%:]')\n",
        "    \n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "  dataset = dataset.map(preprocessing) # Apply data preprocessing\n",
        "  dataset = dataset.prefetch(-1)       # fetch next batches while training current one (-1 for autotune)\n",
        "  return dataset"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSC4323pMr1U"
      },
      "source": [
        "## Building a simple regression model with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoS-O2jDR4In"
      },
      "source": [
        "import tensorflow.keras as tfk\n",
        "\n",
        "def create_model():\n",
        "  model = tfk.models.Sequential()\n",
        "\n",
        "  # .... Add your CNN model here\n",
        "  \n",
        "  model.compile(optimizer='adam', # learning rate will be set by LearningRateScheduler\n",
        "                loss=# .... Add a loss here\n",
        "                )\n",
        "  return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SiLbwK6PrW2"
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "# print model layers\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgppIA1sdZe6"
      },
      "source": [
        "# We actually create our training dataset with our input function\n",
        "dataset_training = input_fn('train')\n",
        "dataset_testing = input_fn('test')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNwxfD0bEjSP"
      },
      "source": [
        "# And we start tensoboard to track our training\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir='./logs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mgZfTZc7zIO"
      },
      "source": [
        "# Learning rate schedule\n",
        "LEARNING_RATE=0.001\n",
        "LEARNING_RATE_EXP_DECAY=0.9\n",
        "lr_decay = tfk.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\n",
        "    verbose=True)\n",
        "\n",
        "# Tensoboard tracking\n",
        "tb_callback = tf.keras.callbacks.TensorBoard('./logs/run1', update_freq='batch')\n",
        "\n",
        "# We are ready to train our model\n",
        "model.fit(dataset_training,\n",
        "          validation_data=dataset_testing,\n",
        "          steps_per_epoch=20000//64,\n",
        "          epochs=10,\n",
        "          callbacks=[lr_decay, tb_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNrIw7y74uEL"
      },
      "source": [
        "# Now that the model is 'trained', we can apply it\n",
        "dataset_eval = input_fn('test', BATCH_SIZE)\n",
        "preds = model.predict(dataset_eval)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFKx1CeMCznI"
      },
      "source": [
        "# Retrieving specz for testing set\n",
        "ground_truth = []\n",
        "for im, z in input_fn('test', BATCH_SIZE):\n",
        "  ground_truth.append(z)\n",
        "ground_truth = np.concatenate(ground_truth, axis=0)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrdAenDnlDmC"
      },
      "source": [
        "hist2d(ground_truth, preds.squeeze(), 64,\n",
        "       range=[[0,1],[0,1]], cmap='gist_stern'); \n",
        "gca().set_aspect('equal');\n",
        "plot([0,1],[0,1],color='r')\n",
        "xlabel('Spectroscopic Redshift')\n",
        "ylabel('Predicted Redshift');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIlXHZcxH-l3"
      },
      "source": [
        "Yes! It's learning something! But the results may not be perfect yet. Here  is a list of things to do to improve the model:\n",
        "*   Train for longer ^^' Deep Learning requires a lot of patience\n",
        "*   Include additional information, like the extinction in each band, which is included in the dataset. \n",
        "    *   You would need to change the input_fn so that the returned dataset contains additional fields\n",
        "    *   See how to use multiple inputs in a Keras model here: https://keras.io/getting-started/functional-api-guide/\n",
        "*   Use a better CNN model. A simple CNN like this one is typically very suboptimal. Much better results would be achieved with a ResNet model for instance, see here for a Keras example: https://keras.io/examples/cifar10_resnet/\n",
        "*   Move away from an MSE loss and embrace TensorflowProbability \\o/ Replacing the output of the model by a Mixture Density would be a much better idea. https://www.tensorflow.org/probability/api_docs/python/tfp/layers/MixtureNormal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho45ddXjJS6d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
