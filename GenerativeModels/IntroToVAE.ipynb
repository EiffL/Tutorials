{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANF_DL_IntroToVAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1KYmcqnlO203eEZLXP+uR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EiffL/Tutorials/blob/master/GenerativeModels/IntroToVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p0j4ua5Nfw0"
      },
      "source": [
        "# Introduction to Variational Auto-Encoders\n",
        "\n",
        "\n",
        "Author: [@EiffL](https://github.com/EiffL) (Francois Lanusse)\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this tutorial, we will progressively learn how to build a Variational Auto-Encoder starting from a classical Auto-Encoder. We will use simple convolutional architecture on the MNIST dataset, the goal being to understand all of the basic mechanisms.\n",
        "\n",
        "Learning objectives:\n",
        "  - Use TensorFlow Dataset to load MNIST digits\n",
        "  - Use Keras to build and train an Auto-Encoder (AE)\n",
        "  - Build insight of AE latent spaces, indentifying the limitations of this  model  \n",
        "  - Learn how to use TensorFlow Probability probabilistic layers\n",
        "  - Use Keras & TFP to build a Varriational Auto-Encoder (VAE)\n",
        "  - Sample new digits from trained VAE\n",
        "\n",
        "\n",
        "### Instructions for enabling GPU access\n",
        "\n",
        "By default, notebooks are started without acceleration. To make sure that the runtime is configured for using GPUs, go to `Runtime > Change runtime type`, and select GPU in `Hardware Accelerator`.\n",
        "\n",
        "### Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "451HhhZuNbzg"
      },
      "source": [
        "%pylab inline\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "tfpl = tfp.layers\n",
        "tfkl = tf.keras.layers\n",
        "tfd = tfp.distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liVqlKYbG49z"
      },
      "source": [
        "### Checking for GPU access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzW4FzQpHAJO"
      },
      "source": [
        "#Checking for GPU access\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjeovcPaHR9w"
      },
      "source": [
        "## Loading data and creating an Input Pipeline\n",
        "\n",
        "Our first step will be load the MNIST dataset using the extremely convenient library [TensorFlow Datasets](https://www.tensorflow.org/datasets). All sorts of common datasets are directly available through that library and can be accessed in just one line. You can see the full list of available datastets [here](https://www.tensorflow.org/datasets/catalog).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgpHPGMOHC20"
      },
      "source": [
        "mnist_dset = tfds.load(name=\"mnist\", # Name of the dataset\n",
        "                       split=\"train\") # split, \"train\" or \"test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_2PQ4ujIjyd"
      },
      "source": [
        "This creates an instance of `tf.data.Datasets`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltlUUxi0Iils"
      },
      "source": [
        "mnist_dset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOWAroPzIvtR"
      },
      "source": [
        "we see that the dataset contains a dictionary with images of size (28,28,1) of type int8 and an associated label. \n",
        "Examples can be drawn from the dataset like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUerItW9JWMe"
      },
      "source": [
        "for example in mnist_dset.take(1): # We take only one example from the dataset \n",
        "  print(\"Our example contains the following keys\", example.keys())\n",
        "  imshow(example['image'][:,:,0],cmap='gray'); colorbar()\n",
        "  title(\"%d\"%example['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbq109upJOMj"
      },
      "source": [
        "\n",
        "For our purpose of generative modeling, we only need to grab the image, we don't care about thte label, and we are going to preprocess these images as floats and rescale thembetween 0 and 1. This can all be done by a preprocessing function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3gUeRTcbCkY"
      },
      "source": [
        "def normalize_img(example):\n",
        "  \"\"\" Preprocessing function that rescales an image between 0,1\n",
        "  This pre-processing function will return twice the image because for an \n",
        "  autoencoder the target is the same as the input.\n",
        "  \"\"\"\n",
        "  im = tf.cast(example['image'], tf.float32) / 255.\n",
        "  return im, im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6DiVaF5Kj_O"
      },
      "source": [
        "We can now create a full input pipeline for our \n",
        "dataset using this pre-processing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhTfG47sPBhj"
      },
      "source": [
        "dset = mnist_dset.map(normalize_img) # Apply the pre-processing function\n",
        "dset = dset.cache()                  # Cache the results\n",
        "dset = dset.shuffle(60000)           # Shuffle the data over a given buffer \n",
        "dset = dset.batch(128)               # Batch the data\n",
        "dset = dset.prefetch(tf.data.experimental.AUTOTUNE) # Pre-fetch the data in parrallel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ea0RqM9LJH0"
      },
      "source": [
        "To learn more about how to use the `tf.data.Datasets` API, check out [this documentation](https://www.tensorflow.org/guide/data).\n",
        "\n",
        "Let's sample a batch from our newly created dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXHikPq1PITt"
      },
      "source": [
        "for im, target in dset.take(1):\n",
        "  print(\"We now have a batch of images of size\", im.shape)\n",
        "  imshow(im[0,:,:,0],cmap='gray'); colorbar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsJufOXtQW5O"
      },
      "source": [
        "## Building a Keras Auto-Encoder\n",
        "\n",
        "\n",
        "Now that we have access to some data, our first goal will be to create a Convolutional Auto-Encoder, which can compress images (in our case of size 28x28) down to some low dimensional latent representation (for instance 2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A4RTb7yQ5pY"
      },
      "source": [
        "### Building an encoder\n",
        "\n",
        "We begin with the encoder. We want to build a function that can create an encoder to compress images down to some dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amfirmaHPpEr"
      },
      "source": [
        "def get_encoder(latent_dim=2):\n",
        "  \"\"\" Creates a small convolutional encoder for the requested latent dimension\n",
        "  \"\"\"\n",
        "  return tf.keras.Sequential([ \n",
        "      tfkl.Input(shape=(28,28,1)),\n",
        "      tfkl.Conv2D(32, kernel_size=3, activation='relu', strides=2, padding='same'),\n",
        "      tfkl.Conv2D(64, kernel_size=3, activation='relu', strides=2, padding='same'),\n",
        "      tfkl.Flatten(),\n",
        "      tfkl.Dense(latent_dim)\n",
        "      ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zv66YngTby_"
      },
      "source": [
        "encoder = get_encoder() # Instantiate a Keras model using our function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBehSP2_Qfma"
      },
      "source": [
        "This has instantiated our encoder, we print out a summary of the model like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNxAO73vP5sh"
      },
      "source": [
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf--BG7fQqiA"
      },
      "source": [
        "We see that through the different layers of the model, the tensors change as follows:\n",
        " - images 28x28x1 at the input level (no shown)\n",
        " - images 14x14x32\n",
        " - images 7x7x64\n",
        " - vector 3136\n",
        " - vector 2 at the output level\n",
        "\n",
        "Even though the model is not trained yet, we can already transform images with the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfl4ZvqISNkv"
      },
      "source": [
        "for batch_im, batch_target in dset.take(1): # Sample only one batch of images\n",
        "  batch_encoded = encoder(batch_im)         # Apply the encoder on images \n",
        "\n",
        "# And we recover the encoding for all images of the batch\n",
        "print(batch_encoded.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeN8ajeySbOb"
      },
      "source": [
        "scatter(batch_encoded[:,0], batch_encoded[:,1])\n",
        "xlabel('z1')\n",
        "ylabel('z2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9j9Y5ksSAGS"
      },
      "source": [
        "\n",
        "### Building a decoder\n",
        "\n",
        "The next step is to build an decoder that mirrors the encoder and transforms a vector of low dimensionality back to an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj6Qe_BTS8ru"
      },
      "source": [
        "def get_decoder(latent_dim=2):\n",
        "  \"\"\" Creates a small convolutional decoder for the requested latent dimension\n",
        "  \"\"\"\n",
        "  return tf.keras.Sequential([\n",
        "      tfkl.Input(shape=(latent_dim,)),\n",
        "      tfkl.Dense(7*7*64, activation='relu'),\n",
        "      tfkl.Reshape((7,7,64)),\n",
        "      tfkl.Conv2DTranspose(64, kernel_size=3, activation='relu', strides=2, padding='same'),\n",
        "      tfkl.Conv2DTranspose(32, kernel_size=3, activation='relu', strides=2, padding='same'),\n",
        "      tfkl.Conv2DTranspose(1, kernel_size=3, activation='sigmoid', strides=1, padding='same')                 \n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIpIBo6jVciS"
      },
      "source": [
        "decoder = get_decoder() # Instantiate decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZDYfMCwVgJo"
      },
      "source": [
        "decoder.summary() # Print out the summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CwMyQThUZah"
      },
      "source": [
        "We see that now the following happens in the model:\n",
        "\n",
        " - vector 2 at the output level (not shown)\n",
        " - vector 3136\n",
        " - images 7x7x64\n",
        " - images 14x14x64\n",
        " - images 28x28x32\n",
        " - images 28x28x1 at the output\n",
        "\n",
        "This more or less reflects our encoder but in reverse.\n",
        "\n",
        "Although not trained yet, we can already run the model to decode our encoded images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0YUwLaNVDB-"
      },
      "source": [
        "batch_decoded = decoder(batch_encoded) # Runs the decoder on our previously \n",
        "                                       # encoded images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vvL4KrZVQ-3"
      },
      "source": [
        "And just for fun, we can try to see how the decoded images look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjleAI5bVDFe"
      },
      "source": [
        "subplot(121)\n",
        "imshow(batch_im[0,:,:,0],cmap='gray'); title('Input image')\n",
        "subplot(122)\n",
        "imshow(batch_decoded[0,:,:,0],cmap='gray'); title('Auto-Encoded image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5SwupgTVk5M"
      },
      "source": [
        "Unsurprisingly, we get a bunch of garbage :-D. Let's try to do some training and see what happens after that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-jz1hWKVzdk"
      },
      "source": [
        "### Training the Auto-Encoder\n",
        "\n",
        "We want to train the encoder and decoder simulateously so that they learn the identity, i.e. decoder(encoder(x)) = x.\n",
        "\n",
        "To do this, let's define a new Keras model that just concatenate both models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iLXsDaWVC9v"
      },
      "source": [
        "auto_encoder = tf.keras.Sequential([\n",
        "      tfkl.InputLayer([28,28,1]),                              \n",
        "      encoder,\n",
        "      decoder])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lars0nlWTZ8"
      },
      "source": [
        "This has created an auto-encoder by concatenation of individual models. To see what has happened we  can look at the model summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhhTR8NAXUDn"
      },
      "source": [
        "auto_encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LLusajlW05J"
      },
      "source": [
        "This model does the following:\n",
        "- images 28x28x1 at the input (not shown)\n",
        "- vector 2\n",
        "- images 28x28x1 at the output\n",
        "\n",
        "The last step is to \"compile\" the Keras model, i.e. specifying an optimizer and a loss function. \n",
        "\n",
        "We are going to use the extremely popular `Adam` optimizer. As a loss function, since our data is binary (0 and 1), we are going to use a binary cross entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6yEWefYXZqb"
      },
      "source": [
        "auto_encoder.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                    loss=tf.keras.losses.binary_crossentropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbotFDoaXtRW"
      },
      "source": [
        "Now that the model is compiled, it can be  fitted to the data using the `.fit()` method. We will just have to provide our dataset and Keras will take care of the rest. \n",
        "\n",
        "Note that our dataset returns tuples of data `(batch_im,  batch_target)`, this is interpreted by Keras as `batch_im` being the input of the model, and the second entry in the tuple `batch_target` being the desired output that the model should learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1W1R_KoYlow"
      },
      "source": [
        "history = auto_encoder.fit(dset, epochs=20)  # Starts training both encoder and decoder\n",
        "                                             # for 20 epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8pcKJ-4ZJHg"
      },
      "source": [
        "And that's it, our model should more or less be trained by now. We can check the model history to see what the loss function looks like as a function of training epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks32aGHVZJf_"
      },
      "source": [
        "plot(history.history['loss'])\n",
        "xlabel('epoch')\n",
        "ylabel('reconstruction loss');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz6fE-KZZpVd"
      },
      "source": [
        "And much more interestingly, we can apply the model on a batch of images and see what comes out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqyRWTZ-Zwvf"
      },
      "source": [
        "codes = encoder(batch_im)\n",
        "decoded_images = decoder(codes)\n",
        "# Here we use the encoder/decoder separately but we could do just the same:\n",
        "decoded_images = auto_encoder(batch_im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vudlZsXGaQm5"
      },
      "source": [
        "figure(figsize=(15,5))\n",
        "subplot(131)\n",
        "imshow(batch_im[0,:,:,0],cmap='gray')\n",
        "title('First input image of batch')\n",
        "subplot(132)\n",
        "scatter(codes[:,0], codes[:,1])\n",
        "scatter(codes[0,0], codes[0,1])\n",
        "title('Latent encoding of batch')\n",
        "subplot(133)\n",
        "imshow(decoded_images[0,:,:,0],cmap='gray')\n",
        "title('Decoded image');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qiurCVEaOZM"
      },
      "source": [
        "### Exploring the Auto-Encoder\n",
        "\n",
        "In this last sub-section, we will try to build a little bit more insight into the Auto-Encoder, its latent space, and how it is behaving. This will serve as motivation for going to Variational Auto-Encoders.\n",
        "\n",
        "\n",
        "We begin by defining a new dataset using the `test` split of the MNIST data, and this time we also want to keep the labels of each digit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnUzJJ-xmVMh"
      },
      "source": [
        "mnist_test_dset = tfds.load(name=\"mnist\",\n",
        "                            split='test')\n",
        "\n",
        "def normalize_img_test(example):\n",
        "  \"\"\" Normalize images, like during training, but also returns label\n",
        "  \"\"\"\n",
        "  im = tf.cast(example['image'], tf.float32) / 255.\n",
        "  return im, example['label']\n",
        "\n",
        "# We build a simplified pipeline for testing\n",
        "dset_test = mnist_test_dset.map(normalize_img_test)\n",
        "dset_test = dset_test.batch(1024) # We use a large batch of 1024 examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BouQdNF0Zs-E"
      },
      "source": [
        "for batch_im, batch_labels in dset_test.take(1):\n",
        "  # This extracts one batch of the test dset and shows the first example\n",
        "  imshow(batch_im[0,:,:,0],cmap='gray')\n",
        "  title('This is a %d'%batch_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G61K6C3Zdal_"
      },
      "source": [
        "#### Auto-Encoding quality\n",
        "\n",
        "Let us compare  input and output images for a few examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azi6HlMmdvz4"
      },
      "source": [
        "autoencoded_im = auto_encoder(batch_im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7LNZ-vqerHb"
      },
      "source": [
        "Let's first draw a few images from the input dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvK7ro1EebBp"
      },
      "source": [
        "figure(figsize=(5,5))\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    subplot(4,4, i*4+j+1)\n",
        "    imshow(batch_im[i*4+j,:,:,0],cmap='gray')\n",
        "    axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfrBJcgGi2Ia"
      },
      "source": [
        "And let's see how the model is able to represent these images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgGDXVYcd6a0"
      },
      "source": [
        "figure(figsize=(5,5))\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    subplot(4,4, i*4+j+1)\n",
        "    imshow(autoencoded_im[i*4+j,:,:,0],cmap='gray')\n",
        "    axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOcY002Ei98B"
      },
      "source": [
        "We recognize more or less the digits but the quality is not excellent, what is interesting is that sometimes the representation is more semantic than a reconsrtuction, i.e. a 0 gets auto-encoded as a 0, but not necessarily as the same 0.\n",
        "\n",
        "The main reason why the quality is not excellent is that the model is not powerful enough to map 28x28 down to 2 without losing information. There would be 2 solutions to improve on this:\n",
        " - Implement a more complex auto-encoder (more layers)\n",
        " - Increase the dimensionality of the latent space, to make the problem simpler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbrnvBCGc6Jg"
      },
      "source": [
        "#### Visualizing the latent space\n",
        "\n",
        "We can now have a look at how different digits are encoded in the latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqV37VVJecej"
      },
      "source": [
        "codes = encoder(batch_im) # Encodes the images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x86Ea5cSfQu5"
      },
      "source": [
        "scatter(codes[:,0],codes[:,1],c=batch_labels,cmap='tab10'); colorbar() # Plot the encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Z2SQXzj9s3"
      },
      "source": [
        "We see that the model tries to naturally place different digits in different regions of latent space without overlapping too much. \n",
        "\n",
        "But we see that the overall distribution of codes is not regular, it has gaps, arbitrary extent, and weird and non-trivial shapes. We can try to sample this latent space on a regular grid and see what the learned manifold looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry1cVoCwf0hT"
      },
      "source": [
        "n = 30\n",
        "scale = 2.\n",
        "# Find out mean and std of encoding\n",
        "x_mean, x_std = np.mean(codes[:,0]), np.std(codes[:,0])\n",
        "y_mean, y_std = np.mean(codes[:,1]), np.std(codes[:,1])\n",
        "# Create uniform grid\n",
        "grid_x = np.linspace(x_mean - scale*x_std, x_mean + scale*x_std, n)\n",
        "grid_y = np.linspace(y_mean - scale*y_std, y_mean + scale*x_std, n)[::-1]\n",
        "# Reshape into batch of coordinates\n",
        "batch_latents = np.stack(meshgrid(grid_x,grid_y),axis=-1)\n",
        "batch_latents = batch_latents.reshape((-1,2))\n",
        "# Run through decoder\n",
        "batch_samples = tf.reshape(decoder(batch_latents), (30,30,28,28))\n",
        "# Reshape into one giant image\n",
        "fig = batch_samples.numpy().transpose((0,2,1,3)).reshape((30*28,30*28))\n",
        "\n",
        "# Plot the figure with corresponding latent ticks\n",
        "figure(figsize=(10, 10))\n",
        "imshow(fig, cmap='gray')\n",
        "start_range = 28 // 2\n",
        "end_range = n * 28 + start_range \n",
        "pixel_range = np.arange(start_range, end_range, 28)\n",
        "sample_range_x = np.round(grid_x, 1)\n",
        "sample_range_y = np.round(grid_y, 1)\n",
        "xticks(pixel_range, sample_range_x)\n",
        "yticks(pixel_range, sample_range_y)\n",
        "xlabel(\"z[0]\");\n",
        "ylabel(\"z[1]\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJvG_WV1pGHz"
      },
      "source": [
        "#### Trying to sample new digits\n",
        "\n",
        "Now that we have an embedding, we can *try* to use and create a simple generative model. For this, we will look at the distribution of latent space codes and randomly sample new points in a similar distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwHdExMgnahA"
      },
      "source": [
        "hist(codes[:,0], 64, label='z0', alpha=0.6);\n",
        "hist(codes[:,1], 64, label='z1', alpha=0.6);\n",
        "legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_7FrTCip1-h"
      },
      "source": [
        "We see that the distribution of latent variables is very irregular, but we can still try to fit it with a Gaussian, and then samples new digits from that Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83-Jm0AmgyUR"
      },
      "source": [
        "z_mu = np.median(codes,axis=0); z_std = np.std(codes,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIQAPu8aqZ2G"
      },
      "source": [
        "# Here we sample some new digits\n",
        "latent_samples = z_mu +  z_std * randn(16,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX8KxjQoqZxN"
      },
      "source": [
        "# Let's see where our new samples fall in the latent distribution\n",
        "scatter(codes[:,0],codes[:,1],c=batch_labels,cmap='tab10'); colorbar()\n",
        "scatter(latent_samples[:,0], latent_samples[:,1], marker='+',c='r');\n",
        "# The red points are our new samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMeiUJgOrdvo"
      },
      "source": [
        "# and now we can decode them\n",
        "batch_samples = tf.reshape(decoder(latent_samples), (4,4,28,28))\n",
        "# Reshape into one giant image\n",
        "fig = batch_samples.numpy().transpose((0,2,1,3)).reshape((4*28,4*28))\n",
        "# Let's see the result\n",
        "imshow(fig, cmap='gray'); axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I73uLCLsH3t"
      },
      "source": [
        "This is not too bad! We recognize some digits, and they are roughly of similar quality as the \n",
        "auto-encoded results. **This means that the latent space is fairly regular**, we can sample from it without too much care, but the quality of samples is not great."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7ECknbRs2gN"
      },
      "source": [
        "#### Increasing the latent space dimensionality\n",
        "\n",
        "As mentioned previously, one option to improve the quality of samples is to increase the dimensionality of the latent space. In this section we are going to try to increase it to 10, and see two things: \n",
        "  - How is the image quality affected\n",
        "  - How is the regularity of the latent space affected\n",
        "\n",
        "First step is to create a new auto-encoder with larger latent space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXFltYLHsHSI"
      },
      "source": [
        "encoder = get_encoder(10)\n",
        "decoder = get_decoder(10)\n",
        "auto_encoder = tf.keras.Sequential([\n",
        "    tfkl.InputLayer([28,28,1]),\n",
        "    encoder,\n",
        "    decoder])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvr4sOayrc9m"
      },
      "source": [
        "# Let's check how our new auto-encoder looks like\n",
        "auto_encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN569Fi9t9Ww"
      },
      "source": [
        "We see that our new encoder has a latent space of dimension 10.\n",
        "\n",
        "Just as before, let's train it on our MNIST training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBfcGDYmrczK"
      },
      "source": [
        "auto_encoder.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                    loss=tf.keras.losses.binary_crossentropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY4vFweKuT9W"
      },
      "source": [
        "history10 = auto_encoder.fit(dset, epochs=20)  # Starts training both encoder and decoder\n",
        "                                             # for 20 epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf_416MSuXDq"
      },
      "source": [
        "plot(history.history['loss'], label='latent_dim=2')\n",
        "plot(history10.history['loss'], label='latent_dim=10')\n",
        "xlabel('epoch')\n",
        "ylabel('reconstruction loss')\n",
        "legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBXg7OnHucN9"
      },
      "source": [
        "We see that the loss now goes much lower than before. This is because the auto-encoder can preserve more information about the input at the latent level.\n",
        "\n",
        "Let's see what the reconstruction quality looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_2Kjc9rvIf1"
      },
      "source": [
        "autoencoded_im = auto_encoder(batch_im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mlMJOfWvIJa"
      },
      "source": [
        "# These are the input images\n",
        "figure(figsize=(5,5))\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    subplot(4,4, i*4+j+1)\n",
        "    imshow(batch_im[i*4+j,:,:,0],cmap='gray')\n",
        "    axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3dLd9pUvH-T"
      },
      "source": [
        "# These are the reconstructed images\n",
        "figure(figsize=(5,5))\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    subplot(4,4, i*4+j+1)\n",
        "    imshow(autoencoded_im[i*4+j,:,:,0],cmap='gray')\n",
        "    axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKEMyx_ivq8A"
      },
      "source": [
        "![](https://media1.tenor.com/images/c7b80e4cf9004b58ad9f3cfd5a3ab345/tenor.gif?itemid=15669873)\n",
        "\n",
        "This is a looooot better than before :-D Great!\n",
        "\n",
        "Now, let's see what the latent space looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US4ckDuawVuV"
      },
      "source": [
        "codes = encoder(batch_im) # Encodes the images\n",
        "print(\"Our latent representation now has dimension d = %d\"%codes.shape[1])\n",
        "# Just like before, we can have a look at the latent space encoding, for instance\n",
        "# along the 2 first dimensions\n",
        "scatter(codes[:,0],codes[:,1],c=batch_labels,cmap='tab10'); colorbar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn-ivN9kw-H6"
      },
      "source": [
        "Things are different compared to the d=2 case:    \n",
        "  - The overall distribution of latent samples seems a lot more regular, i.e. looks like a Gaussian. \n",
        "  - The encoding for different digits appear to be overlapping.\n",
        "\n",
        "So... **what will happen if we try to fit a Gaussian model to this latent space and sample from it?**, just like we did before in the previous section? Let's find out  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5gQutlkwbJ0"
      },
      "source": [
        "z_mu = np.median(codes,axis=0); z_std = np.std(codes,axis=0)\n",
        "latent_samples = z_mu +  z_std * randn(16,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmDZjtSNx4Ot"
      },
      "source": [
        "# Let's see where our new samples fall in the latent distribution\n",
        "scatter(codes[:,0],codes[:,1],c=batch_labels,cmap='tab10'); colorbar()\n",
        "scatter(latent_samples[:,0], latent_samples[:,1], marker='+',c='r',s=100);\n",
        "# The red points are our new samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRTfFnb7yI7v"
      },
      "source": [
        "# and now we can decode them\n",
        "batch_samples = tf.reshape(decoder(latent_samples), (4,4,28,28))\n",
        "# Reshape into one giant image\n",
        "fig = batch_samples.numpy().transpose((0,2,1,3)).reshape((4*28,4*28))\n",
        "# Let's see the result\n",
        "imshow(fig, cmap='gray'); axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRHRmbVpyYQz"
      },
      "source": [
        "We get some digits.... but also some garbage...\n",
        "\n",
        "![](https://media1.tenor.com/images/b386fbb5c9c59b3f7d690e6cdc9bb8fb/tenor.gif?itemid=14214249)\n",
        "\n",
        "The quality of these samples is far from the quality of reconstructed images. \n",
        "\n",
        "**What we have gained in quality of auto-encoding, we have lost in regularity of latent space!** We can no longer sample decent digits just by drawing from a Gaussian.\n",
        "\n",
        "Ideally, we would want a way to train the model with a penalty that would force it to make the latent space look like a Gaussian. This is exactly what a Variational Auto-Encoder does!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAmCMwEPzgj-"
      },
      "source": [
        "## Building a Keras & TFP Variational Auto-Encoder\n",
        "\n",
        "Having built some insight on what happens in an auto-encoder at the previous section, we will now try to improve on a simple Auto-Encoder by implementing a Variational Auto-Encoder.\n",
        "\n",
        "A key library that we will use in this section is the **excellent** [TensorFlow Probabilty](https://www.tensorflow.org/probability) library. In particular we are going to use the TFP probabilistic keras layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JITZMs1q0evn"
      },
      "source": [
        "### Implementing the recognition model\n",
        "\n",
        "In the VAE framework, the encoder is also known as a `recognition model`, it is very similar to our traditional encoder, but instead of outputing a code, it outputs a **distribution over possible codes**, a.k.a a posterior distribution. We will ask explicitly to the model to penalize departures of this distributions from a standard Gaussian.\n",
        "\n",
        "Here is how we can modify our original encoder with some TFP magic to turn the output into a distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm0pwjHo1Mb3"
      },
      "source": [
        "def get_probabilistic_encoder(latent_dim=2):\n",
        "  \"\"\" Creates a small convolutional encoder for the requested latent dimension\n",
        "  \"\"\"\n",
        "  # We choose a prior distribution for the latent codes\n",
        "  prior = tfd.MultivariateNormalDiag(loc=tf.zeros(latent_dim))\n",
        "\n",
        "  return tf.keras.Sequential([ \n",
        "      tfkl.Input(shape=(28,28,1)),\n",
        "      tfkl.Conv2D(32, kernel_size=3, activation='relu', strides=2, padding='same'),\n",
        "      tfkl.Conv2D(64, kernel_size=3, activation='relu', strides=2, padding='same'),\n",
        "      tfkl.Flatten(),\n",
        "      tfkl.Dense(128, activation='relu'),\n",
        "      # We ask this layer to output a vector of size equal to the number of\n",
        "      # parameters required to define a Multivariate Gaussian\n",
        "      tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim)),\n",
        "      # At the last layer, we ask the model to output a **distribution**\n",
        "      # In this case, a Multivariate Normal\n",
        "      tfpl.MultivariateNormalTriL(latent_dim, \n",
        "              # And we specify a regularization for this distribution, used\n",
        "              # during training, we want the KL divergence with the prior \n",
        "              # to be small, i.e. the encoded distribution should be close to a \n",
        "              # standard Gaussian\n",
        "              activity_regularizer=tfpl.KLDivergenceRegularizer(prior))\n",
        "      ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnkyFqWq36yn"
      },
      "source": [
        "Let's try to instantiate this encoder, and encode some images, to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vq2RfkQ1L82"
      },
      "source": [
        "prob_encoder = get_probabilistic_encoder(latent_dim=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knl2zWD-1Lpi"
      },
      "source": [
        "prob_encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9QWa2E44vXG"
      },
      "source": [
        "for batch_im, batch_target in dset.take(1): # Sample only one batch of images\n",
        "  batch_encoded = prob_encoder(batch_im)         # Apply the encoder on images "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYhS16SS5Ct9"
      },
      "source": [
        "Let's inspect what `batch_encoded` is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPUBa6WY1K7g"
      },
      "source": [
        "batch_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaZ8clVx5Gqm"
      },
      "source": [
        "We see that this is an instance of a `tfp.distributions.MultivariateNormalTriL`, this is a distribution!\n",
        "\n",
        "We can manipulate it in different ways:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awpEbTLf5XRj"
      },
      "source": [
        "# We can draw samples from it\n",
        "batch_encoded.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07g2zaT65XC_"
      },
      "source": [
        "# We can retrieve the mean\n",
        "batch_encoded.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0PA747A54Hj"
      },
      "source": [
        "### Implementing the generator\n",
        "\n",
        "Now that we have a recognition model, we want to implemenent the VAE equivalent of the decoder, aka the `generator`.\n",
        "\n",
        "The model will be very similar to the decoder, but the difference is that we are going to need to assume a likelihood $p(x | z)$ for our generator. In the case of binary data, an obvious choice is to use a [Bernoulli Distribution](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Bernoulli).\n",
        "\n",
        "Here is how we can do that using TFP magic again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDcCrF9H60i2"
      },
      "source": [
        "def get_probabilistic_decoder(latent_dim=2):\n",
        "  \"\"\" Creates a small convolutional decoder for the requested latent dimension\n",
        "  \"\"\"\n",
        "  return tf.keras.Sequential([\n",
        "      tfkl.Input(shape=(latent_dim,)),\n",
        "      tfkl.Dense(7*7*64, activation='relu'),\n",
        "      tfkl.Reshape((7,7,64)),\n",
        "      tfkl.Conv2DTranspose(64, kernel_size=3, activation='relu', strides=2, padding='same'),\n",
        "      tfkl.Conv2DTranspose(32, kernel_size=3, activation='relu', strides=2, padding='same'),\n",
        "      tfkl.Conv2DTranspose(1, kernel_size=3, activation=None, strides=1, padding='same') ,\n",
        "      tfkl.Flatten(),\n",
        "      # We ask the model to output a Bernoulli distribution with shape [28x28x1]\n",
        "      tfpl.IndependentBernoulli([28,28,1])                \n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQRhHDNS60M7"
      },
      "source": [
        "# Let's instantiate the decoder\n",
        "prob_decoder = get_probabilistic_decoder(latent_dim=10)\n",
        "# And check its summary\n",
        "prob_decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I585EpaV8SQk"
      },
      "source": [
        "And we see that the model outputs a Bernoulli distribution, so a distribution of images, not a single image. \n",
        "\n",
        "Let's try to decode a random sample of our encoded images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyhT-McK8mLt"
      },
      "source": [
        "# Draw a radom sample of the code\n",
        "code_sample = batch_encoded.sample()\n",
        "# And decode that sample\n",
        "decoded_im = prob_decoder(code_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW3RdPvi87NB"
      },
      "source": [
        "And let's inspect what we obtain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tm2Vjqf866N"
      },
      "source": [
        "decoded_im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bUnIrix9Anj"
      },
      "source": [
        "This is again a distribution, so for instance, we might want to retrieve the mean, or a random sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qym2imYT9Ibm"
      },
      "source": [
        "figure(figsize=(9,3))\n",
        "subplot(131)\n",
        "imshow(batch_im[0,:,:,0],cmap='gray'); axis('off')\n",
        "title(\"Input Image\")\n",
        "subplot(132)\n",
        "imshow(decoded_im.sample()[0,:,:,0],cmap='gray'); axis('off')\n",
        "title(\"Sample from generator output\")\n",
        "subplot(133)\n",
        "imshow(decoded_im.mean()[0,:,:,0],cmap='gray'); axis('off')\n",
        "title(\"Mean of generator output\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwhAlHK--A_c"
      },
      "source": [
        "### Putting it all together: building a VAE\n",
        "\n",
        "Now that we have a *regularized* recognition model and a generator, we can combine them into a single Keras VAE.\n",
        "\n",
        "\n",
        "Let's start by building the model, by concatenating both models:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azdKSS52-5E8"
      },
      "source": [
        "vae = tf.keras.Sequential([\n",
        "          tfkl.InputLayer([28,28,1]),\n",
        "          prob_encoder,\n",
        "          prob_decoder])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWx7GZNc_GL8"
      },
      "source": [
        "vae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1cd_n5O_MDo"
      },
      "source": [
        "Let's just check what happens if we feed the VAE a same batch of images several times:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0k6AOcx_SUk"
      },
      "source": [
        "figure(figsize=(9,3))\n",
        "subplot(131)\n",
        "samples = vae(batch_im)\n",
        "imshow(samples.mean()[0,:,:,0],cmap='gray'); axis('off');\n",
        "title('run 1');\n",
        "subplot(132)\n",
        "samples = vae(batch_im)\n",
        "imshow(samples.mean()[0,:,:,0],cmap='gray'); axis('off');\n",
        "title('run 2');\n",
        "subplot(133)\n",
        "samples = vae(batch_im)\n",
        "imshow(samples.mean()[0,:,:,0],cmap='gray'); axis('off');\n",
        "title('run 3');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdd9Oz0T_8iW"
      },
      "source": [
        "We obtain different images, because every times we run the VAE model, a different sample from latent space is used.\n",
        "\n",
        "Ok, let's try to train the model, so that we can do more interesting things. We need to compile it with appropriate losses. The KL divergence on the recognition model will automatically be applied, as we specified it during construction. We will just need to tell Keras how to compute the *reconstruction loss* i.e. the likelihood of the input data under the generator. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Vy6BYtAklw"
      },
      "source": [
        "# We define the reconstruction loss as the negative log likelihood\n",
        "negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
        "# And use it to compile the VAE\n",
        "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
        "            loss=negloglik)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PW1jrYUBA6G"
      },
      "source": [
        "Ok, now we train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_nqYv69BAbz"
      },
      "source": [
        "historyVAE = vae.fit(dset, epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H3q4LOOAka9"
      },
      "source": [
        "plot(historyVAE.history['loss']);\n",
        "ylabel('ELBO')\n",
        "xlabel('epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWs5ywlXB66c"
      },
      "source": [
        "Ok, neat, it's training. Let's try to see an example of auto-encoding on the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTJ9ZknTByHw"
      },
      "source": [
        "for batch_im, batch_labels in dset_test.take(1):\n",
        "  # This extracts one batch of the test dset and shows the first example\n",
        "  imshow(batch_im[0,:,:,0],cmap='gray')\n",
        "  title('This is a %d'%batch_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQG_JUF0EOZ7"
      },
      "source": [
        "autoencoded_im = vae(batch_im) # Run the input batch through the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYccesmfBxuI"
      },
      "source": [
        "subplot(121)\n",
        "# Plot the mean of the output Bernoulli distribution\n",
        "imshow(autoencoded_im.mean()[0,:,:,0],cmap='gray'); axis('off'); \n",
        "title('Mean output')\n",
        "subplot(122)\n",
        "# Plot a random sample of the output Bernoulli distribution\n",
        "imshow(autoencoded_im.sample()[0,:,:,0],cmap='gray'); axis('off');\n",
        "title('Sample output');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox5NDeDKDeJ5"
      },
      "source": [
        "We see that the model has indeed learned to auto-encode images. The plot above illustrates once again that the output of the model is a distribution, we may choose to look at its mean, or a sample from it.\n",
        "\n",
        "To assess the  general quality of the auto-encoded images, let's look at a few examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrJ1J2moEAct"
      },
      "source": [
        "# These are the input images\n",
        "figure(figsize=(5,5))\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    subplot(4,4, i*4+j+1)\n",
        "    imshow(batch_im[i*4+j,:,:,0],cmap='gray')\n",
        "    axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dng12RP_D_7l"
      },
      "source": [
        "# These are the reconstructed images\n",
        "figure(figsize=(5,5))\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    subplot(4,4, i*4+j+1)\n",
        "    imshow(autoencoded_im.mean()[i*4+j,:,:,0],cmap='gray')\n",
        "    axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVICtmshEVtV"
      },
      "source": [
        "Pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olTM81b6EaPR"
      },
      "source": [
        "#### Sampling from the generative model\n",
        "\n",
        "The main reason for using a VAE is that we can use it as a proper generative model. For that, we draw from the latent space prior, and forward the samples through the generator.\n",
        "\n",
        "Let's start by sampling some latent codes from the prior:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv-r51HWEZyJ"
      },
      "source": [
        "latent_samples = tfd.MultivariateNormalDiag(loc=tf.zeros(10)).sample(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8qA-CoLFLN4"
      },
      "source": [
        "Then, we forward these samples through the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqPFD4r9D_we"
      },
      "source": [
        "image_samples = prob_decoder(latent_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCTyJhq1FjjK"
      },
      "source": [
        "# Grab the mean images, and reshape them into one giant image\n",
        "fig = image_samples.mean().numpy().reshape((4,4,28,28))\n",
        "fig = fig.transpose((0,2,1,3)).reshape((4*28,4*28))\n",
        "# Let's see the result\n",
        "imshow(fig, cmap='gray'); axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G943Km7AGdAh"
      },
      "source": [
        "And that's it! We have sampled new digits from a VAE. Compared to what we obtained in the AE case, these are a lot less garbage than before, illustrating that the latent space of the VAE is a lot more regular, even in the case d=10.\n",
        "\n",
        "Also note that we didn't even have to look at the distrbution of the  latent space to draw these samples. Thanks to the KL regularization, the latent space naturally tries to follow a standard Gaussian. We will take a look at that in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byX0qvaKHUKE"
      },
      "source": [
        "#### Investigating the VAE latent space\n",
        "\n",
        "Let's check what the latent space of the VAE looks like. First, let's encode some images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZUoWVLz-U00"
      },
      "source": [
        "codes = prob_encoder(batch_im) # Run the input batch through the model\n",
        "codes_smpl = codes.sample() # Remember, latent codes are distributions, we draw one example\n",
        "\n",
        "# Just like before, we can have a look at the latent space encoding, for instance\n",
        "# along the 2 first dimensions\n",
        "scatter(codes_smpl[:,0], codes_smpl[:,1], \n",
        "        c=batch_labels,cmap='tab10'); colorbar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLkuNCxvI13E"
      },
      "source": [
        "This looks very Gaussian, we can also look at the marginal distributions along each latent space axes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URqAN9fiH9zu"
      },
      "source": [
        "for i in range(10):\n",
        "  hist(codes_smpl[:,i], 64, range=[-3,3],alpha=0.2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSAkCaK0JRmJ"
      },
      "source": [
        "All of our latent space dimensions look Gaussian as expected. One last thing that we can look at, is the latent space distribution predicted by the recognition model for a single image:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43q4PQl5JJh2"
      },
      "source": [
        "codes_smpls = codes.sample(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnmyzLAMKlOU"
      },
      "source": [
        "# The shape of these samples is [n_samples, batch_size, d]\n",
        "codes_smpls.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "866tMbIQJluj"
      },
      "source": [
        "figure(figsize=[5,5])\n",
        "\n",
        "subplot(221)\n",
        "imshow(batch_im[0,:,:,0],cmap='gray'); axis('off')\n",
        "title('im 1')\n",
        "subplot(222)\n",
        "imshow(batch_im[1,:,:,0],cmap='gray'); axis('off')\n",
        "title('im 2')\n",
        "subplot(223)\n",
        "hist2d(codes_smpls[:,0,0], codes_smpls[:,0,1],64, range=[[-3,3],[-3,3]]); gca().set_aspect('equal');\n",
        "xlabel('z0')\n",
        "ylabel('z1')\n",
        "title('posterior im 1')\n",
        "subplot(224)\n",
        "hist2d(codes_smpls[:,1,0], codes_smpls[:,1,1],64, range=[[-3,3],[-3,3]]); gca().set_aspect('equal');\n",
        "xlabel('z0')\n",
        "ylabel('z1')\n",
        "title('posterior im 2');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NQa9LcbK5N4"
      },
      "source": [
        "This plot shows that 2 different **images get encoded into entire regions** of latent space. We are only looking at the first two dimensions of the latent space here, but you can have a look a the other dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRpk28D_Le6T"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook we have highlighted the fundamental difference between an auto-encoder and variational auto-encoder, that is the regularisation of latent space.\n",
        "\n",
        "We have seen all of the fundamentals of how to build a VAE. To go beyond this toy example, you would just need to add more convolution layers to both encoder and decoder, but everything else remains the same.\n"
      ]
    }
  ]
}